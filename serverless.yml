app: backend
service: backend
frameworkVersion: '2'

# [1] exclude `package.json` files from being bundled.
plugins:
  - serverless-appsync-plugin
  - serverless-iam-roles-per-function
  - serverless-export-env # (6) integration test
  - serverless-prune-plugin # fixes Code storage limit exceeded error https://github.com/muratkeremozcan/appsyncmasterclass-backend/actions/runs/4303926516/jobs/7504249742#step:8:49
  # - serverless-manifest-plugin # [10] get API_URL from CognitoUserPoolArn # not needed as of 78
  # (103) Implement sampling for resolver logs
  - serverless-plugin-ifelse

provider:
  name: aws
  runtime: nodejs12.x
  region: eu-west-1
  # [105] Configure Xray tracing
  # (105.0) add tracing to to provider
  tracing:
    lambda: true
    apiGateway: false
  iamRoleStatements:
    - Effect: Allow
      Action:
        - xray:PutTraceSegments
        - xray:PutTelemetryRecords
      Resource: '*'

  versionFunctions: false # avoids Code storage limit exceeded error https://github.com/muratkeremozcan/appsyncmasterclass-backend/actions/runs/4303926516/jobs/7504249742#step:8:49
  # can also do automatic pruning https://github.com/claygregory/serverless-prune-plugin#automatic-pruning, but we really don't need versions here...

  # stage: dev # this is default, we don't have to specify it
  # any env var defined here applies to all the functions
  environment:
    # (4.2) when using aws nodeJs SDK, always enable HTTP keep-alive
    STAGE: ${self:custom.stage} # stage: dev # if not specified, defaults to dev
    AWS_NODEJS_CONNECTION_REUSE_ENABLED: '1'

package:
  exclude:
    - package-lock.json
    - package.json
    - '**/test-helpers/**'
    - '**/__tests__/**'

custom:
  # (6) add AWS_REGION as an env var (use region from CLI command override, otherwise provider:region:)
  region: ${opt:region, self:provider.region}
  # (4.0) Environment is dev, unless we pass in a stage override; npm run sls -- -s prod
  stage: ${opt:stage, self:provider.stage}
  # [1] Create a separate `serverless.appsync-api.yml` file for AppSync configuration.
  appSync: ${file(serverless.appsync-api.yml)}
  # (10) get API_URL from CognitoUserPoolArn # not needed as of 78
  # manifest:
  #   postProcess: ./processManifest.js
  #   disablePostDeployGeneration: true
  #   disableOutput: true
  #   silent: true

  # if you want to use pruning lambda versions with serverless-prune-plugin, instead of setting versionFunctions: false
  # prune:
  #   automatic: true
  #   includeLayers: true
  #   number: 3

  # Define the custom.stage variable with a default value of 'dev'
  custom:
    stage: ${self:custom.stage, 'dev'}

  # [89] Per resolver caching
  # AWS X-ray shows lots of redundant requests for user profile. Although they are
  # in parallel, the DDB queries cost. We can cache some of these requests,
  # especially nested queries, to reduce costs. We decide on what to cache based on
  # this stream of data.

  # (89.0) define caching in serverless.yml
  # appSyncCaching:
  #   default:
  #   prod:
  #     behavior: PER_RESOLVER_CACHING
  #     ttl: 3600
  #     type: T2_SMALL # you pay for it so keep it small

  # [102] Configure AppSync logging
  # (102.0) add a custom log level
  appSyncLogLevel:
    default: ALL
    prod: ERROR

  # [103] Implement sampling for resolver logs\
  serverlessIfElse:
    - If: '"${self:custom.stage}" == "prod"'
      ElseExclude:
        - functions.setResolverLogLevelToAll
        - functions.setResolverLogLevelToError

functions:
  # (4.1) Add a functions block for the lambda trigger function
  confirmUserSignup:
    handler: functions/confirm-user-signup.handler
    # the function needs to know the name of the UsersTable, which is generated by CloudFormation
    # this one is a function level env var, they aggregate over provider: environment:
    environment:
      USERS_TABLE: !Ref UsersTable
    # the function needs the permission to write to the UsersTable
    # note: we don't want a global iamRoleStatements: under provider: , we just want permission for this function
    # (4.2) we use npm i -D serverless-iam-roles-per-function to do this
    # auto generated role name for function too long, so we use iamRoleStatementsName to override it
    iamRoleStatementsName: ${self:service}-${self:custom.stage}-confirmUserSignup
    iamRoleStatements:
      - Effect: Allow
        Action: dynamodb:PutItem # in DDB, Put means rest POST, and Update means rest PUT
        Resource: !GetAtt UsersTable.Arn

  # (15.1) add the lambda function that will do the work (getImageUploadUrl)
  getImageUploadUrl:
    handler: functions/get-upload-url.handler
    # (15.2) create the S3 bucket env var, to help make the s3 putObject request
    environment:
      BUCKET_NAME: !Ref AssetsBucket
    # auto generated role name for function too long, so we use iamRoleStatementsName to override it
    iamRoleStatementsName: ${self:service}-${self:custom.stage}-getImageUploadUrl
    iamRoleStatements:
      - Effect: Allow
        Action:
          - s3:PutObject # the lambda needs the S3 putObject permission
          - s3:PutObjectAcl # it also needs ACL permission because we set it in the params (get-upload-url.js/s3.getSignedUrl('putObject', params))
        Resource: !Sub ${AssetsBucket.Arn}/* # allow the function to interact with any object in the bucket

  # (17.2.1) add the yml for the lambda function that will generate a tweet `ulid` for the 3 DDB tables,
  # write to Tweets and Timelines tables, and update Users table.
  tweet:
    handler: functions/tweet.handler
    environment: # we need to transact with 3 DDB tables
      USERS_TABLE: !Ref UsersTable
      TWEETS_TABLE: !Ref TweetsTable
      TIMELINES_TABLE: !Ref TimelinesTable
    iamRoleStatements:
      # in DDB, Put means rest POST, and Update means rest PUT
      - Effect: Allow # we need to update the tweet count at UsersTable
        Action: dynamodb:UpdateItem #
        Resource: !GetAtt UsersTable.Arn
      - Effect: Allow # we need to write to TweetsTable and TimelinesTable
        Action: dynamodb:PutItem
        Resource:
          - !GetAtt TweetsTable.Arn
          - !GetAtt TimelinesTable.Arn

  # (35.1) add a new function for retweet
  # We need to add an entry to the `TweetsTable` for the retweet, which means we need a tweetId, which is a `ulid`
  # ulid requires us to use a lambda resolver. Similar to (17.2).
  # Get from Tweets, Update Tweets and Users, write to Tweets, Timelines, Retweets,
  retweet:
    handler: functions/retweet.handler
    environment:
      USERS_TABLE: !Ref UsersTable
      TWEETS_TABLE: !Ref TweetsTable
      TIMELINES_TABLE: !Ref TimelinesTable
      RETWEETS_TABLE: !Ref RetweetsTable
    iamRoleStatements:
      - Effect: Allow
        Action: dynamodb:GetItem
        Resource: !GetAtt TweetsTable.Arn
      - Effect: Allow
        Action: dynamodb:UpdateItem
        Resource:
          - !GetAtt TweetsTable.Arn
          - !GetAtt UsersTable.Arn
      - Effect: Allow
        Action: dynamodb:PutItem
        Resource:
          - !GetAtt TweetsTable.Arn
          - !GetAtt TimelinesTable.Arn
          - !GetAtt RetweetsTable.Arn

  # (44.2) Add the lambda for reply
  # Get from Tweets, Update Tweets and Users, write to Tweets, Timelines
  # Similar to (35.1) retweets without the retweet table
  reply:
    handler: functions/reply.handler
    environment:
      USERS_TABLE: !Ref UsersTable
      TWEETS_TABLE: !Ref TweetsTable
      TIMELINES_TABLE: !Ref TimelinesTable
    iamRoleStatements:
      - Effect: Allow
        Action: dynamodb:GetItem
        Resource: !GetAtt TweetsTable.Arn
      - Effect: Allow
        Action: dynamodb:UpdateItem
        Resource:
          - !GetAtt TweetsTable.Arn
          - !GetAtt UsersTable.Arn
      - Effect: Allow
        Action: dynamodb:PutItem
        Resource:
          - !GetAtt TweetsTable.Arn
          - !GetAtt TimelinesTable.Arn

  # (39.2) add a new function for unretweet, almost the same as retweet at (35.1)
  unretweet:
    handler: functions/unretweet.handler
    environment:
      USERS_TABLE: !Ref UsersTable
      TWEETS_TABLE: !Ref TweetsTable
      TIMELINES_TABLE: !Ref TimelinesTable
      RETWEETS_TABLE: !Ref RetweetsTable
    iamRoleStatements:
      - Effect: Allow
        Action: dynamodb:GetItem
        Resource: !GetAtt TweetsTable.Arn
      # we have to query DDB for the retweet so that we can delete it
      # we use CloudFormation's !Sub to interpolate the ARN of the table
      - Effect: Allow
        Action: dynamodb:Query
        Resource: !Sub '${TweetsTable.Arn}/index/retweetsByCreator'
      - Effect: Allow
        Action: dynamodb:UpdateItem
        Resource:
          - !GetAtt TweetsTable.Arn
          - !GetAtt UsersTable.Arn
      - Effect: Allow
        Action: dynamodb:DeleteItem
        Resource:
          - !GetAtt TweetsTable.Arn
          - !GetAtt TimelinesTable.Arn
          - !GetAtt RetweetsTable.Arn

  # [51] Distribute tweets to followers
  # (51.1) add a new function for distributeTweets
  distributeTweets:
    handler: functions/distribute-tweets.handler
    environment:
      RELATIONSHIPS_TABLE: !Ref RelationshipsTable
      TIMELINES_TABLE: !Ref TimelinesTable
    events: # lambda triggered by a stream event
      - stream:
          type: dynamodb
          arn: !GetAtt TweetsTable.StreamArn
    # auto generated role name for function too long, so we use iamRoleStatementsName to override it
    iamRoleStatementsName: ${self:service}-${self:custom.stage}-distributeTweets
    iamRoleStatements:
      - Effect: Allow
        Action:
          - dynamodb:PutItem
          - dynamodb:DeleteItem
          - dynamodb:BatchWriteItem
        Resource: !GetAtt TimelinesTable.Arn
      - Effect: Allow
        Action: dynamodb:Query
        Resource: !Sub '${RelationshipsTable.Arn}/index/byOtherUser'

  # [54]Implement add tweets to timeline when following someone
  # (54.0)  add the lambda and enable streams on the table it's streaming from.
  distributeTweetsToFollower:
    handler: functions/distribute-tweets-to-follower.handler
    environment:
      TWEETS_TABLE: !Ref TweetsTable
      TIMELINES_TABLE: !Ref TimelinesTable
      MAX_TWEETS: '100'
    events: # lambda triggered by a stream event
      - stream:
          type: dynamodb
          arn: !GetAtt RelationshipsTable.StreamArn
    # (56) workaround to 64 character limit
    # auto generated role name for function too long, so we use iamRoleStatementsName to override it
    iamRoleStatementsName: ${self:service}-${self:custom.stage}-distributeTweetsToFollower
    iamRoleStatements:
      - Effect: Allow
        Action: dynamodb:Query
        Resource:
          - !Sub '${TweetsTable.Arn}/index/byCreator'
          - !Sub '${TimelinesTable.Arn}/index/byDistributedFrom'
      - Effect: Allow
        Action:
          - dynamodb:BatchWriteItem
          - dynamodb:PutItem
          - dynamodb:DeleteItem
        Resource: !GetAtt TimelinesTable.Arn

  # [64] Sync users and tweets to Algolia; we need to get all our DDB data into Algolia so that we can search them.
  # (64.1) Listen in on the stream of events from tweets & users tables, then sync the updates to Algolia. Similar to distributeTweets (51.1)
  syncUsersToAlgolia:
    handler: functions/sync-users-to-algolia.handler
    events:
      - stream:
          type: dynamodb
          arn: !GetAtt UsersTable.StreamArn
    iamRoleStatementsName: ${self:service}-${self:custom.stage}-syncUsersToAlgolia
    # [65] Securely handle secrets
    # (65.1) instead of using plain env vars, get the values from AWS SSM Parameter Store
    iamRoleStatements:
      - Effect: Allow
        Action: ssm:GetParameters
        Resource:
          - !Sub arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/${self:custom.stage}/algolia-app-id
          - !Sub arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/${self:custom.stage}/algolia-admin-key

  # (64.1)
  syncTweetsToAlgolia:
    handler: functions/sync-tweets-to-algolia.handler
    events:
      - stream:
          type: dynamodb
          arn: !GetAtt TweetsTable.StreamArn
    iamRoleStatementsName: ${self:service}-${self:custom.stage}-syncTweetsToAlgolia
    # [65] Securely handle secrets
    # (65.1) instead of using plain env vars, get the values from AWS SSM Parameter Store
    iamRoleStatements:
      - Effect: Allow
        Action: ssm:GetParameters
        Resource:
          - !Sub arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/${self:custom.stage}/algolia-app-id
          - !Sub arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/${self:custom.stage}/algolia-admin-key

  # [67] Implement search query
  # (67.0) add a lambda for search handler
  search:
    handler: functions/search.handler
    iamRoleStatementsName: ${self:service}-${self:custom.stage}-search
    iamRoleStatements:
      - Effect: Allow
        Action: ssm:GetParameters
        Resource:
          - !Sub arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/${self:custom.stage}/algolia-app-id
          - !Sub arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/${self:custom.stage}/algolia-admin-key

  # [70] Implement getHashTag query
  # (70.0) add a lambda for getHashTag handler
  getHashTag:
    handler: functions/get-hash-tag.handler
    iamRoleStatementsName: ${self:service}-${self:custom.stage}-getHashTag
    iamRoleStatements:
      - Effect: Allow
        Action: ssm:GetParameters
        Resource:
          - !Sub arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/${self:custom.stage}/algolia-app-id
          - !Sub arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/${self:custom.stage}/algolia-admin-key

  # [74]  Add subscription for retweets
  # Use a DDB stream to trigger a lambda function whenever a tweet is retweeted, then send the notifyRetweeted mutation to AppSync API.
  # (74.0) add a lambda function for notify handler, similar to distributeTweets (51.1)
  notify:
    handler: functions/notify.handler
    environment:
      GRAPHQL_API_URL: !GetAtt GraphQlApi.GraphQLUrl
      TWEETS_TABLE: !Ref TweetsTable
      USERS_TABLE: !Ref UsersTable
    events:
      - stream:
          type: dynamodb
          arn: !GetAtt TweetsTable.StreamArn
    iamRoleStatementsName: ${self:service}-${self:custom.stage}-notify
    iamRoleStatements:
      - Effect: Allow
        Action: appsync:GraphQL
        Resource: !Sub ${GraphQlApi.Arn}/*
      - Effect: Allow
        Action: dynamodb:GetItem
        Resource: !GetAtt TweetsTable.Arn
      - Effect: Allow
        Action: dynamodb:Query
        Resource: !Sub ${UsersTable.Arn}/index/byScreenName

  # [75] Add subscription for likes
  # Use a DDB stream to trigger a lambda function whenever a tweet is liked, then send the notifyLiked mutation to AppSync API.
  # (75.0) add a lambda function for liked handler, similar to distributeTweets (74.0)
  notifyLiked:
    handler: functions/notify-liked.handler
    environment:
      GRAPHQL_API_URL: !GetAtt GraphQlApi.GraphQLUrl
      TWEETS_TABLE: !Ref TweetsTable
    events:
      - stream:
          type: dynamodb
          arn: !GetAtt LikesTable.StreamArn
    iamRoleStatementsName: ${self:service}-${self:custom.stage}-notifyLiked
    iamRoleStatements:
      - Effect: Allow
        Action: appsync:GraphQL
        Resource: !Sub ${GraphQlApi.Arn}/*
      - Effect: Allow
        Action: dynamodb:GetItem
        Resource: !GetAtt TweetsTable.Arn

  notifyDmed:
    handler: functions/notify-dmed.handler
    environment:
      GRAPHQL_API_URL: !GetAtt GraphQlApi.GraphQLUrl
    events:
      - stream:
          type: dynamodb
          arn: !GetAtt DirectMessagesTable.StreamArn
    iamRoleStatementsName: ${self:service}-${self:custom.stage}-notifyDmed
    iamRoleStatements:
      - Effect: Allow
        Action: appsync:GraphQL
        Resource: !Sub ${GraphQlApi.Arn}/*

  # [82] Implement sendDirectMessage mutation
  # (82.0) add a lambda function that handles the sendDirectMessage handler
  sendDirectMessage:
    handler: functions/send-direct-message.handler
    environment:
      CONVERSATIONS_TABLE: !Ref ConversationsTable
      DIRECT_MESSAGES_TABLE: !Ref DirectMessagesTable
    iamRoleStatementsName: ${self:service}-${self:custom.stage}-sendDirectMessage
    iamRoleStatements:
      - Effect: Allow
        Action: dynamodb:PutItem
        Resource: !GetAtt DirectMessagesTable.Arn
      - Effect: Allow
        Action: dynamodb:UpdateItem
        Resource: !GetAtt ConversationsTable.Arn

  # [90] BatchInvoke to reduce the umber of lambda invocations
  # (90.0) Add the lambda function to `serverless.yml`
  getTweetCreator:
    handler: functions/get-tweet-creator.handler
    environment:
      USERS_TABLE: !Ref UsersTable
    iamRoleStatementsName: ${self:service}-${self:custom.stage}-getTweetCreator
    iamRoleStatements:
      - Effect: Allow
        Action: dynamodb:BatchGetItem
        Resource: !GetAtt UsersTable.Arn

  # (93.1) define the lambda function
  firehoseTransformer:
    handler: functions/firehose-transformer.handler
    timeout: 61 # must set this timeout to avoid a console warning

  # (103) Implement sampling for resolver logs
  setResolverLogLevelToAll:
    handler: functions/set-resolver-log-level.handler
    events:
      - schedule: cron(6 * * * ? *) # 6 mins past the hour every hour
    environment:
      APPSYNC_API_ID: !GetAtt GraphQlApi.ApiId
      FIELD_LOG_LEVEL: ALL
    iamRoleStatementsName: ${self:service}-${self:custom.stage}-setLogLevelToAll
    iamRoleStatements:
      - Effect: Allow
        Action:
          - appsync:GetGraphqlApi
          - appsync:UpdateGraphqlApi
        Resource: !Ref GraphQlApi
      - Effect: Allow
        Action: iam:PassRole
        Resource: !GetAtt AppSyncLoggingServiceRole.Arn
  setResolverLogLevelToError:
    handler: functions/set-resolver-log-level.handler
    events:
      - schedule: cron(12 * * * ? *) # 12 mins past the hour every hour
    environment:
      APPSYNC_API_ID: !GetAtt GraphQlApi.ApiId
      FIELD_LOG_LEVEL: ERROR
    iamRoleStatementsName: ${self:service}-${self:custom.stage}-setLogLevelToErr
    iamRoleStatements:
      - Effect: Allow
        Action:
          - appsync:GetGraphqlApi
          - appsync:UpdateGraphqlApi
        Resource: !Ref GraphQlApi
      - Effect: Allow
        Action: iam:PassRole
        Resource: !GetAtt AppSyncLoggingServiceRole.Arn

resources:
  Resources:
    # [4] Save user profile on PostConfirmation
    # (4.0) Create a DynamoDB table to store user profiles
    UsersTable:
      Type: AWS::DynamoDB::Table
      Properties:
        BillingMode: PAY_PER_REQUEST
        KeySchema:
          - AttributeName: id
            KeyType: HASH
        AttributeDefinitions:
          - AttributeName: id
            AttributeType: S
          # (49.1) Add `screenName` as  global secondary index to `UsersTable`
          - AttributeName: screenName
            AttributeType: S
        GlobalSecondaryIndexes:
          - IndexName: byScreenName
            KeySchema:
              - AttributeName: screenName
                KeyType: HASH
            Projection:
              ProjectionType: ALL
        StreamSpecification:
          StreamViewType: NEW_AND_OLD_IMAGES
        Tags:
          - Key: Environment
            # (4.0) Environment is dev, unless we pass in a stage override; npm run sls -- -s prod
            Value: ${self:custom.stage}
            # a helper tag to aid in monitoring the individual cost of dynamodb tables
          - Key: Name
            Value: users-table

    # [17] Implement tweet mutation
    # (17.0) Create a DynamoDB table to store tweets
    TweetsTable:
      Type: AWS::DynamoDB::Table
      Properties:
        BillingMode: PAY_PER_REQUEST
        KeySchema:
          - AttributeName: id
            KeyType: HASH
        AttributeDefinitions:
          # to fetch the tweets for a particular user, we also need the DDB partition key
          - AttributeName: id # sort key
            AttributeType: S
          - AttributeName: creator # partition key
            AttributeType: S
          - AttributeName: retweetOf # (39.2) add a new index for retweets
            AttributeType: S
          - AttributeName: inReplyToTweetId
            AttributeType: S # (43.2) add a new index for replies
        GlobalSecondaryIndexes:
          - IndexName: byCreator
            KeySchema:
              - AttributeName: creator # partition key
                KeyType: HASH
              - AttributeName: id # sort key
                KeyType: RANGE
            Projection:
              ProjectionType: ALL # so that when we get the tweets, we get all items
          - IndexName: retweetsByCreator # (39.2) add a new index for retweets
            KeySchema:
              - AttributeName: creator
                KeyType: HASH
              - AttributeName: retweetOf
                KeyType: RANGE
            Projection:
              ProjectionType: ALL
          - IndexName: repliesForTweet # (43.2) add a new index for replies
            KeySchema:
              - AttributeName: inReplyToTweetId
                KeyType: HASH
              - AttributeName: id
                KeyType: RANGE
            Projection:
              ProjectionType: ALL
        # (51.0) enable Dynamo stream specification on tweets table, used to trigger a lambda function
        StreamSpecification:
          StreamViewType: NEW_AND_OLD_IMAGES
        Tags: # so that we can track the cost in AWS billing
          - Key: Environment
            # same as (4.0) Environment is dev, unless we pass in a stage override; npm run sls -- -s prod
            Value: ${self:custom.stage}
          # a helper tag to aid in monitoring the individual cost of dynamodb tables
          - Key: Name
            Value: tweets-table

    # (17.1) Create a DynamoDB table to store tweet timelines
    TimelinesTable:
      Type: AWS::DynamoDB::Table
      Properties:
        BillingMode: PAY_PER_REQUEST
        KeySchema:
          - AttributeName: userId # partition key
            KeyType: HASH
          - AttributeName: tweetId # sort key
            KeyType: RANGE
        AttributeDefinitions:
          - AttributeName: userId
            AttributeType: S
          - AttributeName: tweetId
            AttributeType: S
          # (54.1) add a global secondary index
          # for the tweets distributed from the followed user
          - AttributeName: distributedFrom
            AttributeType: S
        GlobalSecondaryIndexes:
          - IndexName: byDistributedFrom
            KeySchema:
              - AttributeName: userId
                KeyType: HASH
              - AttributeName: distributedFrom
                KeyType: RANGE
            Projection:
              ProjectionType: ALL
        Tags:
          - Key: Environment
            Value: ${self:custom.stage}
          - Key: Name
            Value: timelines-table

    # [26] like mutation
    # (26.0) create a new DDB table to track which user has liked which tweet
    LikesTable:
      Type: AWS::DynamoDB::Table
      Properties:
        BillingMode: PAY_PER_REQUEST
        KeySchema:
          - AttributeName: userId # partition key
            KeyType: HASH
          - AttributeName: tweetId # sort key
            KeyType: RANGE
        AttributeDefinitions:
          - AttributeName: userId
            AttributeType: S
          - AttributeName: tweetId
            AttributeType: S
        StreamSpecification:
          StreamViewType: NEW_AND_OLD_IMAGES
        Tags: # so that we can track the cost in AWS billing
          - Key: Environment
            # same as (4.0) Environment is dev, unless we pass in a stage override; npm run sls -- -s prod
            Value: ${self:custom.stage}
          # a helper tag to aid in monitoring the individual cost of dynamodb tables
          - Key: Name
            Value: likes-table

    # [35] Implement retweet mutation
    # (35.0) create a new DDB table to track which user has retweeted which tweet
    RetweetsTable:
      Type: AWS::DynamoDB::Table
      Properties:
        BillingMode: PAY_PER_REQUEST
        KeySchema:
          - AttributeName: userId
            KeyType: HASH
          - AttributeName: tweetId
            KeyType: RANGE
        AttributeDefinitions:
          - AttributeName: userId
            AttributeType: S
          - AttributeName: tweetId
            AttributeType: S
        Tags:
          - Key: Environment
            Value: ${self:custom.stage}
          - Key: Name
            Value: retweets-table

    # [47] Implement follow mutation
    # (47.0) create a relationships table to track which user follows/blocks/etc. who
    # sk for sort key
    RelationshipsTable:
      Type: AWS::DynamoDB::Table
      Properties:
        BillingMode: PAY_PER_REQUEST
        KeySchema:
          - AttributeName: userId
            KeyType: HASH
          - AttributeName: sk
            KeyType: RANGE
        AttributeDefinitions:
          - AttributeName: userId
            AttributeType: S
          - AttributeName: sk
            AttributeType: S
          - AttributeName: otherUserId
            AttributeType: S
        GlobalSecondaryIndexes:
          - IndexName: byOtherUser
            KeySchema:
              - AttributeName: otherUserId
                KeyType: HASH
              - AttributeName: sk
                KeyType: RANGE
            Projection:
              ProjectionType: ALL
        # (54.0) add the lambda and enable streams on the table it's streaming from.
        StreamSpecification:
          StreamViewType: NEW_AND_OLD_IMAGES
        Tags:
          - Key: Environment
            Value: ${self:custom.stage}
          - Key: Name
            Value: relationships-table

    # (74.1) Create a DynamoDB table to store notifications
    NotificationsTable:
      Type: AWS::DynamoDB::Table
      Properties:
        BillingMode: PAY_PER_REQUEST
        KeySchema:
          - AttributeName: id
            KeyType: HASH
          - AttributeName: userId
            KeyType: RANGE
        AttributeDefinitions:
          - AttributeName: id
            AttributeType: S
          - AttributeName: userId
            AttributeType: S
        GlobalSecondaryIndexes:
          - IndexName: byUserId
            KeySchema:
              - AttributeName: userId
                KeyType: HASH
              - AttributeName: id
                KeyType: RANGE
            Projection:
              ProjectionType: ALL
        Tags:
          - Key: Environment
            Value: ${self:custom.stage}
          - Key: Name
            Value: notifications-table

    # (82.0.1) add the new tables needed for conversations
    ConversationsTable:
      Type: AWS::DynamoDB::Table
      Properties:
        BillingMode: PAY_PER_REQUEST
        KeySchema:
          - AttributeName: userId
            KeyType: HASH
          - AttributeName: otherUserId
            KeyType: RANGE
        AttributeDefinitions:
          - AttributeName: userId
            AttributeType: S
          - AttributeName: otherUserId
            AttributeType: S
          - AttributeName: lastModified
            AttributeType: S
        GlobalSecondaryIndexes:
          - IndexName: byUserId
            KeySchema:
              - AttributeName: userId
                KeyType: HASH
              - AttributeName: lastModified
                KeyType: RANGE
            Projection:
              ProjectionType: ALL
        Tags:
          - Key: Environment
            Value: ${self:custom.stage}
          - Key: Name
            Value: conversations-table

    DirectMessagesTable:
      Type: AWS::DynamoDB::Table
      Properties:
        BillingMode: PAY_PER_REQUEST
        KeySchema:
          - AttributeName: conversationId
            KeyType: HASH
          - AttributeName: messageId
            KeyType: RANGE
        AttributeDefinitions:
          - AttributeName: conversationId
            AttributeType: S
          - AttributeName: messageId
            AttributeType: S
        StreamSpecification:
          StreamViewType: NEW_AND_OLD_IMAGES
        Tags:
          - Key: Environment
            Value: ${self:custom.stage}
          - Key: Name
            Value: direct-messages-table

    # [3] Configure Cognito User Pool
    CognitoUserPool:
      Type: AWS::Cognito::UserPool
      Properties:
        AutoVerifiedAttributes:
          - email
        Policies:
          PasswordPolicy:
            MinimumLength: 8
            RequireLowercase: false
            RequireNumbers: false
            RequireUppercase: false
            RequireSymbols: false
        # (3.0) allows login via email
        UsernameAttributes:
          - email
        # (3.0) we need to know the name and associate it with cognito user pool
        # with this setting we can configure a name attribute
        Schema:
          - AttributeDataType: String
            Name: name
            Required: false
            Mutable: true
        # (4.3) Configure Cognito to call the lambda trigger function when a new user is registered.
        # We can't use the lambda function's name, because that's something local to serverless framework
        # instead we figure out the logical id sls generates for the lambda function, by using npm run sls -- package
        # which generates cloudformation template under .serverless folder. There look for ConfirmUserSignupLambdaFunction
        LambdaConfig:
          PostConfirmation: !GetAtt ConfirmUserSignupLambdaFunction.Arn

    # (4.4) We also need to give Cognito additional permissions to call the lambda function, by default it doesn't have any
    # grants CognitoUserPool the lambda:invokeFunction permission for ConfirmUserSignupLambdaFunction
    UserPoolInvokeConfirmUserSignupLambdaPermission:
      Type: AWS::Lambda::Permission
      Properties:
        Action: lambda:invokeFunction
        FunctionName: !Ref ConfirmUserSignupLambdaFunction
        Principal: cognito-idp.amazonaws.com
        SourceArn: !GetAtt CognitoUserPool.Arn

    # (3.5) We need to be logged in with Cognito to test AppSync queries.
    # Create a cognito user by hand at *CognitoUserPool / Users and Groups*. (I used my email).
    # We also need to configure a application client at *CognitoUserPool / App clients*
    # to be able to interact with the Cognito User Pool.
    # We do this by adding a resource here
    WebUserPoolClient:
      Type: AWS::Cognito::UserPoolClient
      Properties:
        UserPoolId: !Ref CognitoUserPool
        ClientName: web
        ExplicitAuthFlows:
          - ALLOW_USER_SRP_AUTH
          - ALLOW_USER_PASSWORD_AUTH
          - ALLOW_REFRESH_TOKEN_AUTH
        # with this we get a "wrong pw" in case a user doesn't exist, makes it harder for attackers to find out if the user exists
        PreventUserExistenceErrors: ENABLED

    # (15.2) create the S3 bucket env var, to help make the s3 putObject request
    AssetsBucket:
      Type: AWS::S3::Bucket
      Properties:
        AccelerateConfiguration:
          AccelerationStatus: Enabled # because we used: const s3 = new S3({useAccelerateEndpoint: true})
        CorsConfiguration: # because the UI client needs to make a request
          CorsRules:
            - AllowedMethods:
                - GET
                - PUT
              AllowedOrigins:
                - '*'
              AllowedHeaders:
                - '*'

    # (93.2) Define all the resources Kinesis Firehose
    # S3 bucket, without any public access
    AnalyticsBucket:
      Type: AWS::S3::Bucket
      Properties:
        PublicAccessBlockConfiguration:
          BlockPublicAcls: true
          BlockPublicPolicy: true
          IgnorePublicAcls: true
          RestrictPublicBuckets: true
    # Kinesis firehose delivery stream
    FirehoseStream:
      Type: AWS::KinesisFirehose::DeliveryStream
      Properties:
        DeliveryStreamType: DirectPut
        ExtendedS3DestinationConfiguration:
          BucketARN: !GetAtt AnalyticsBucket.Arn
          BufferingHints:
            IntervalInSeconds: 60
            SizeInMBs: 1
          CloudWatchLoggingOptions:
            Enabled: true
            LogGroupName: !Ref FirehoseLogGroup
            LogStreamName: !Ref FirehoseLogStream
          CompressionFormat: GZIP
          RoleARN: !GetAtt FirehoseDeliveryIamRole.Arn
          Prefix: analytics/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/
          ErrorOutputPrefix: analytics_errors/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/!{firehose:error-output-type}
          ProcessingConfiguration:
            Enabled: true
            Processors:
              - Type: Lambda
                Parameters:
                  - ParameterName: LambdaArn
                    ParameterValue: !GetAtt FirehoseTransformerLambdaFunction.Arn
    # Cloudwatch log group
    FirehoseLogGroup:
      Type: AWS::Logs::LogGroup
      Properties:
        RetentionInDays: 14
    # Cloudwatch log stream within that group
    FirehoseLogStream:
      Type: AWS::Logs::LogStream
      Properties:
        LogGroupName:
          Ref: FirehoseLogGroup
    # IAM role for cloudwatch stream
    FirehoseDeliveryIamRole:
      Type: AWS::IAM::Role
      Properties:
        AssumeRolePolicyDocument:
          Version: '2012-10-17'
          Statement:
            - Effect: Allow
              Principal:
                Service: firehose.amazonaws.com
              Action: sts:AssumeRole
        Path: '/'
        Policies:
          - PolicyName: root
            PolicyDocument:
              Version: '2012-10-17'
              Statement:
                - Effect: Allow
                  Action:
                    - s3:AbortMultipartUpload
                    - s3:GetBucketLocation
                    - s3:GetObject
                    - s3:ListBucket
                    - s3:ListBucketMultipartUploads
                    - s3:PutObject
                  Resource:
                    - !GetAtt AnalyticsBucket.Arn
                    - !Sub ${AnalyticsBucket.Arn}/*
                - Effect: Allow
                  Action: logs:PutLogEvents
                  Resource: !Sub arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:${FirehoseLogGroup}:log-stream:*
                - Effect: Allow
                  Action:
                    - lambda:InvokeFunction
                    - lambda:GetFunctionConfiguration
                  Resource: !GetAtt FirehoseTransformerLambdaFunction.Arn

    # [94] Configure Cognito Identity Pool and IAM role
    IdentityPool:
      Type: AWS::Cognito::IdentityPool
      Properties:
        AllowUnauthenticatedIdentities: true
        AllowClassicFlow: false
        CognitoIdentityProviders:
          - ClientId: !Ref WebUserPoolClient
            ProviderName: !GetAtt CognitoUserPool.ProviderName
            ServerSideTokenCheck: true

    UnauthedClientRole:
      Type: AWS::IAM::Role
      Properties:
        AssumeRolePolicyDocument:
          Version: '2012-10-17'
          Statement:
            - Effect: Allow
              Principal:
                Federated: cognito-identity.amazonaws.com
              Action: sts:AssumeRoleWithWebIdentity
              Condition:
                StringEquals:
                  cognito-identity.amazonaws.com:aud: !Ref IdentityPool
                ForAnyValue:StringLike:
                  cognito-identity.amazonaws.com:amr: unauthenticated
        Policies:
          - PolicyName: CognitoPolicy
            PolicyDocument:
              Version: '2012-10-17'
              Statement:
                - Effect: Allow
                  Action:
                    - firehose:PutRecord
                    - firehose:PutRecordBatch
                  Resource: !GetAtt FirehoseStream.Arn
                # (96.1) add unauthenticated GQL operations
                - Effect: Allow
                  Action: appsync:GraphQL
                  Resource:
                    - !Sub ${GraphQlApi.Arn}/types/Query/fields/getAnalyticsConfig
                    - !Sub ${GraphQlApi.Arn}/types/AnalyticsConfig

    AuthedClientRole:
      Type: AWS::IAM::Role
      Properties:
        AssumeRolePolicyDocument:
          Version: '2012-10-17'
          Statement:
            - Effect: Allow
              Principal:
                Federated: cognito-identity.amazonaws.com
              Action: sts:AssumeRoleWithWebIdentity
              Condition:
                StringEquals:
                  cognito-identity.amazonaws.com:aud: !Ref IdentityPool
                ForAnyValue:StringLike:
                  cognito-identity.amazonaws.com:amr: authenticated
        Policies:
          - PolicyName: CognitoPolicy
            PolicyDocument:
              Version: '2012-10-17'
              Statement:
                - Effect: Allow
                  Action:
                    - firehose:PutRecord
                    - firehose:PutRecordBatch
                  Resource: !GetAtt FirehoseStream.Arn
                # (96.1) add unauthenticated GQL operations
                - Effect: Allow
                  Action: appsync:GraphQL
                  Resource:
                    - !Sub ${GraphQlApi.Arn}/types/Query/fields/getAnalyticsConfig
                    - !Sub ${GraphQlApi.Arn}/types/AnalyticsConfig

    IdentityPoolRoleMapping:
      Type: AWS::Cognito::IdentityPoolRoleAttachment
      Properties:
        IdentityPoolId: !Ref IdentityPool
        Roles:
          authenticated: !GetAtt AuthedClientRole.Arn
          unauthenticated: !GetAtt UnauthedClientRole.Arn

    # (102.1) add the AppSync logging role
    AppSyncLoggingServiceRole:
      Type: AWS::IAM::Role
      Properties:
        AssumeRolePolicyDocument:
          Version: '2012-10-17'
          Statement:
            - Effect: Allow
              Principal:
                Service: appsync.amazonaws.com
              Action: sts:AssumeRole
        Path: /service-role/
        Policies:
          - PolicyName: root
            PolicyDocument:
              Version: '2012-10-17'
              Statement:
                - Effect: Allow
                  Action:
                    - logs:CreateLogGroup
                    - logs:CreateLogStream
                    - logs:PutLogEvents
                  Resource: !Sub arn:aws:logs:${AWS::Region}:${AWS::AccountId}:*

  Outputs:
    # (6) add AWS_REGION as an env var
    # Use the `${self:custom.*}` trick for AWS_REGION, because we cannot use it as lambda function level since that is specific to sls.
    AwsRegion:
      Value: ${self:custom.region}

    # (3.1) We need the CognitoUserPoolId of the CognitoUserPool as a cloud formation output,
    # We get that value with !Ref
    # lets us use process.env.COGNITO_USER_POOL_ID in the e2e test
    CognitoUserPoolId:
      Value: !Ref CognitoUserPool

    # (7) In order to work with cognito in the e2e test and simulate a user signup, we need `WebUserPoolClient` id.
    # We capture that as an output in the `serverless.yml` Outputs section,
    # similar to what we did to acquire COGNITO_USER_POOL_ID (3.1)
    # lets us use process.env.WEB_COGNITO_USER_POOL_CLIENT_ID in the e2e test
    WebCognitoUserPoolClientId:
      Value: !Ref WebUserPoolClient

    # export env gives GRAPHQL_API_URL=[object Object], so we trick serverless yml
    ApiUrl:
      Value: !GetAtt GraphQlApi.GraphQLUrl
