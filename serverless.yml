org: muratkerem
app: appsyncmasterclass-backend
service: appsyncmasterclass-backend
frameworkVersion: '2'

# [1] exclude `package.json` files from being bundled.
plugins:
  - serverless-appsync-plugin
  - serverless-iam-roles-per-function
  - serverless-export-env # (6) integration test
  - serverless-manifest-plugin # [10] get API_URL from CognitoUserPoolArn

provider:
  name: aws
  runtime: nodejs12.x
  region: eu-west-1
  # any env var defined here applies to all the functions
  environment:
    # (4.2) when using aws nodeJs SDK, always enable HTTP keep-alive
    STAGE: ${self:custom.stage} # stage: dev # if not specified, defaults to dev
    AWS_NODEJS_CONNECTION_REUSE_ENABLED: '1'

package:
  exclude:
    - package-lock.json
    - package.json

custom:
  # (6) add AWS_REGION as an env var (use region from CLI command override, otherwise provider:region:)
  region: ${opt:region, self:provider.region}
  # (4.0) Environment is dev, unless we pass in a stage override; npm run sls -- -s prod
  stage: ${opt:stage, self:provider.stage}
  # [1] Create a separate `serverless.appsync-api.yml` file for AppSync configuration.
  appSync: ${file(serverless.appsync-api.yml)}
  # (10) get API_URL from CognitoUserPoolArn
  manifest:
    postProcess: ./processManifest.js
    disablePostDeployGeneration: true
    disableOutput: true
    silent: true

# (4.1) Add a functions block for the lambda trigger function
functions:
  confirmUserSignup:
    handler: functions/confirm-user-signup.handler
    # the function needs to know the name of the UsersTable, which is generated by CloudFormation
    # this one is a function level env var, they aggregate over provider: environment:
    environment:
      USERS_TABLE: !Ref UsersTable
    # the function needs the permission to write to the UsersTable
    # note: we don't want a global iamRoleStatements: under provider: , we just want permission for this function
    # (4.2) we use npm i -D serverless-iam-roles-per-function to do this
    iamRoleStatements:
      - Effect: Allow
        Action: dynamodb:PutItem # in DDB, Put means rest POST, and Update means rest PUT
        Resource: !GetAtt UsersTable.Arn

  # (15.1) add the lambda function that will do the work (getImageUploadUrl)
  getImageUploadUrl:
    handler: functions/get-upload-url.handler
    # (15.2) create the S3 bucket env var, to help make the s3 putObject request
    environment:
      BUCKET_NAME: !Ref AssetsBucket
    iamRoleStatements:
      - Effect: Allow
        Action:
          - s3:PutObject # the lambda needs the S3 putObject permission
          - s3:PutObjectAcl # it also needs ACL permission because we set it in the params (get-upload-url.js/s3.getSignedUrl('putObject', params))
        Resource: !Sub ${AssetsBucket.Arn}/* # allow the function to interact with any object in the bucket

  # (17.2.1) add the yml for the lambda function that will generate a tweet `ulid` for the 3 DDB tables,
  # write to Tweets and Timelines tables, and update Users table.
  tweet:
    handler: functions/tweet.handler
    environment: # we need to transact with 3 DDB tables
      USERS_TABLE: !Ref UsersTable
      TWEETS_TABLE: !Ref TweetsTable
      TIMELINES_TABLE: !Ref TimelinesTable
    iamRoleStatements:
      # in DDB, Put means rest POST, and Update means rest PUT
      - Effect: Allow # we need to update the tweet count at UsersTable
        Action: dynamodb:UpdateItem #
        Resource: !GetAtt UsersTable.Arn
      - Effect: Allow # we need to write to TweetsTable and TimelinesTable
        Action: dynamodb:PutItem
        Resource:
          - !GetAtt TweetsTable.Arn
          - !GetAtt TimelinesTable.Arn

  # (35.1) add a new function for retweet
  # We need to add an entry to the `TweetsTable` for the retweet, which means we need a tweetId, which is a `ulid`
  # ulid requires us to use a lambda resolver. Similar to (17.2).
  retweet:
    handler: functions/retweet.handler
    environment:
      USERS_TABLE: !Ref UsersTable
      TWEETS_TABLE: !Ref TweetsTable
      TIMELINES_TABLE: !Ref 36
      RETWEETS_TABLE: !Ref RetweetsTable
    iamRoleStatements:
      # Get from Tweets, Update Tweets and Users, write to Tweets, Timelines, Retweets,
      - Effect: Allow
        Action: dynamodb:GetItem
        Resource: !GetAtt TweetsTable.Arn
      - Effect: Allow
        Action: dynamodb:UpdateItem
        Resource:
          - !GetAtt TweetsTable.Arn
          - !GetAtt UsersTable.Arn
      - Effect: Allow
        Action: dynamodb:PutItem
        Resource:
          - !GetAtt TweetsTable.Arn
          - !GetAtt TimelinesTable.Arn
          - !GetAtt RetweetsTable.Arn

resources:
  Resources:
    # [4] Save user profile on PostConfirmation
    # (4.0) Create a DynamoDB table to store user profiles
    UsersTable:
      Type: AWS::DynamoDB::Table
      Properties:
        BillingMode: PAY_PER_REQUEST
        KeySchema:
          - AttributeName: id
            KeyType: HASH
        AttributeDefinitions:
          - AttributeName: id
            AttributeType: S
        Tags:
          - Key: Environment
            # (4.0) Environment is dev, unless we pass in a stage override; npm run sls -- -s prod
            Value: ${self:custom.stage}
            # a helper tag to aid in monitoring the individual cost of dynamodb tables
          - Key: Name
            Value: users-table

    # [17] Implement tweet mutation
    # (17.0) Create a DynamoDB table to store tweets
    TweetsTable:
      Type: AWS::DynamoDB::Table
      Properties:
        BillingMode: PAY_PER_REQUEST
        KeySchema:
          - AttributeName: id
            KeyType: HASH
        AttributeDefinitions:
          # to fetch the tweets for a particular user, we also need the DDB partition key
          - AttributeName: creator # partition key
            AttributeType: S
          - AttributeName: id # sort key
            AttributeType: S
        GlobalSecondaryIndexes:
          - IndexName: byCreator
            KeySchema:
              - AttributeName: creator # partition key
                KeyType: HASH
              - AttributeName: id # sort key
                KeyType: RANGE
            Projection:
              ProjectionType: ALL # so that when we get the tweets, we get all items
        Tags: # so that we can track the cost in AWS billing
          - Key: Environment
            # same as (4.0) Environment is dev, unless we pass in a stage override; npm run sls -- -s prod
            Value: ${self:custom.stage}
          # a helper tag to aid in monitoring the individual cost of dynamodb tables
          - Key: Name
            Value: tweets-table

    # (17.1) Create a DynamoDB table to store tweet timelines
    TimelinesTable:
      Type: AWS::DynamoDB::Table
      Properties:
        BillingMode: PAY_PER_REQUEST
        KeySchema:
          - AttributeName: userId # partition key
            KeyType: HASH
          - AttributeName: tweetId # sort key
            KeyType: RANGE
        AttributeDefinitions:
          - AttributeName: userId
            AttributeType: S
          - AttributeName: tweetId
            AttributeType: S
        Tags: # so that we can track the cost in AWS billing
          - Key: Environment
            # same as (4.0) Environment is dev, unless we pass in a stage override; npm run sls -- -s prod
            Value: ${self:custom.stage}
          # a helper tag to aid in monitoring the individual cost of dynamodb tables
          - Key: Name
            Value: timelines-table

    # [3] Configure Cognito User Pool
    CognitoUserPool:
      Type: AWS::Cognito::UserPool
      Properties:
        AutoVerifiedAttributes:
          - email
        Policies:
          PasswordPolicy:
            MinimumLength: 8
            RequireLowercase: false
            RequireNumbers: false
            RequireUppercase: false
            RequireSymbols: false
        # (3.0) allows login via email
        UsernameAttributes:
          - email
        # (3.0) we need to know the name and associate it with cognito user pool
        # with this setting we can configure a name attribute
        Schema:
          - AttributeDataType: String
            Name: name
            Required: false
            Mutable: true
        # (4.3) Configure Cognito to call the lambda trigger function when a new user is registered.
        # We can't use the lambda function's name, because that's something local to serverless framework
        # instead we figure out the logical id sls generates for the lambda function, by using npm run sls -- package
        # which generates cloudformation template under .serverless folder. There look for ConfirmUserSignupLambdaFunction
        LambdaConfig:
          PostConfirmation: !GetAtt ConfirmUserSignupLambdaFunction.Arn

    # (4.4) We also need to give Cognito additional permissions to call the lambda function, by default it doesn't have any
    # grants CognitoUserPool the lambda:invokeFunction permission for ConfirmUserSignupLambdaFunction
    UserPoolInvokeConfirmUserSignupLambdaPermission:
      Type: AWS::Lambda::Permission
      Properties:
        Action: lambda:invokeFunction
        FunctionName: !Ref ConfirmUserSignupLambdaFunction
        Principal: cognito-idp.amazonaws.com
        SourceArn: !GetAtt CognitoUserPool.Arn

    # (3.5) We need to be logged in with Cognito to test AppSync queries.
    # Create a cognito user by hand at *CognitoUserPool / Users and Groups*. (I used my email).
    # We also need to configure a application client at *CognitoUserPool / App clients*
    # to be able to interact with the Cognito User Pool.
    # We do this by adding a resource here
    WebUserPoolClient:
      Type: AWS::Cognito::UserPoolClient
      Properties:
        UserPoolId: !Ref CognitoUserPool
        ClientName: web
        ExplicitAuthFlows:
          - ALLOW_USER_SRP_AUTH
          - ALLOW_USER_PASSWORD_AUTH
          - ALLOW_REFRESH_TOKEN_AUTH
        # with this we get a "wrong pw" in case a user doesn't exist, makes it harder for attackers to find out if the user exists
        PreventUserExistenceErrors: ENABLED

    # (15.2) create the S3 bucket env var, to help make the s3 putObject request
    AssetsBucket:
      Type: AWS::S3::Bucket
      Properties:
        AccelerateConfiguration:
          AccelerationStatus: Enabled # because we used: const s3 = new S3({useAccelerateEndpoint: true})
        CorsConfiguration: # because the UI client needs to make a request
          CorsRules:
            - AllowedMethods:
                - GET
                - PUT
              AllowedOrigins:
                - '*'
              AllowedHeaders:
                - '*'

    # [26] like mutation
    # (26.0) create a new DDB table to track which user has liked which tweet
    LikesTable:
      Type: AWS::DynamoDB::Table
      Properties:
        BillingMode: PAY_PER_REQUEST
        KeySchema:
          - AttributeName: userId # partition key
            KeyType: HASH
          - AttributeName: tweetId # sort key
            KeyType: RANGE
        AttributeDefinitions:
          - AttributeName: userId
            AttributeType: S
          - AttributeName: tweetId
            AttributeType: S
        Tags: # so that we can track the cost in AWS billing
          - Key: Environment
            # same as (4.0) Environment is dev, unless we pass in a stage override; npm run sls -- -s prod
            Value: ${self:custom.stage}
          # a helper tag to aid in monitoring the individual cost of dynamodb tables
          - Key: Name
            Value: likes-table

    # [35] Implement retweet mutation
    # (35.0) create a new DDB table to track which user has retweeted which tweet
    RetweetsTable:
      Type: AWS::DynamoDB::Table
      Properties:
        BillingMode: PAY_PER_REQUEST
        KeySchema:
          - AttributeName: userId
            KeyType: HASH
          - AttributeName: tweetId
            KeyType: RANGE
        AttributeDefinitions:
          - AttributeName: userId
            AttributeType: S
          - AttributeName: tweetId
            AttributeType: S
        Tags:
          - Key: Environment
            Value: ${self:custom.stage}
          - Key: Name
            Value: retweets-table

  Outputs:
    # (3.1) We need the CognitoUserPoolId of the CognitoUserPool as a cloud formation output,
    # We get that value with !Ref
    CognitoUserPoolId:
      Value: !Ref CognitoUserPool

    # (7) In order to work with cognito in the e2e test and simulate a user signup, we need `WebUserPoolClient` id.
    # We capture that as an output in the `serverless.yml` Outputs section,
    # similar to what we did to acquire COGNITO_USER_POOL_ID (3.1)
    WebCognitoUserPoolClientId:
      Value: !Ref WebUserPoolClient

    # (6) add AWS_REGION as an env var
    # Use the `${self:custom.*}` trick for AWS_REGION, because we cannot use it as lambda function level since that is specific to sls.
    AwsRegion:
      Value: ${self:custom.region}
