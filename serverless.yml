app: backend
service: backend
frameworkVersion: '2'

# [1] exclude `package.json` files from being bundled.
plugins:
  - serverless-appsync-plugin
  - serverless-iam-roles-per-function
  - serverless-export-env # (6) integration test
  - serverless-manifest-plugin # [10] get API_URL from CognitoUserPoolArn

provider:
  name: aws
  runtime: nodejs12.x
  region: eu-west-1
  # stage: dev # this is default, we don't have to specify it
  # any env var defined here applies to all the functions
  environment:
    # (4.2) when using aws nodeJs SDK, always enable HTTP keep-alive
    STAGE: ${self:custom.stage} # stage: dev # if not specified, defaults to dev
    AWS_NODEJS_CONNECTION_REUSE_ENABLED: '1'

package:
  exclude:
    - package-lock.json
    - package.json

custom:
  # (6) add AWS_REGION as an env var (use region from CLI command override, otherwise provider:region:)
  region: ${opt:region, self:provider.region}
  # (4.0) Environment is dev, unless we pass in a stage override; npm run sls -- -s prod
  stage: ${opt:stage, self:provider.stage}
  # [1] Create a separate `serverless.appsync-api.yml` file for AppSync configuration.
  appSync: ${file(serverless.appsync-api.yml)}
  # (10) get API_URL from CognitoUserPoolArn
  manifest:
    postProcess: ./processManifest.js
    disablePostDeployGeneration: true
    disableOutput: true
    silent: true

functions:
  # (4.1) Add a functions block for the lambda trigger function
  confirmUserSignup:
    handler: functions/confirm-user-signup.handler
    # the function needs to know the name of the UsersTable, which is generated by CloudFormation
    # this one is a function level env var, they aggregate over provider: environment:
    environment:
      USERS_TABLE: !Ref UsersTable
    # the function needs the permission to write to the UsersTable
    # note: we don't want a global iamRoleStatements: under provider: , we just want permission for this function
    # (4.2) we use npm i -D serverless-iam-roles-per-function to do this
    # auto generated role name for function too long, so we use iamRoleStatementsName to override it
    iamRoleStatementsName: ${self:service}-${self:custom.stage}-confirmUserSignup
    iamRoleStatements:
      - Effect: Allow
        Action: dynamodb:PutItem # in DDB, Put means rest POST, and Update means rest PUT
        Resource: !GetAtt UsersTable.Arn

  # (15.1) add the lambda function that will do the work (getImageUploadUrl)
  getImageUploadUrl:
    handler: functions/get-upload-url.handler
    # (15.2) create the S3 bucket env var, to help make the s3 putObject request
    environment:
      BUCKET_NAME: !Ref AssetsBucket
    # auto generated role name for function too long, so we use iamRoleStatementsName to override it
    iamRoleStatementsName: ${self:service}-${self:custom.stage}-getImageUploadUrl
    iamRoleStatements:
      - Effect: Allow
        Action:
          - s3:PutObject # the lambda needs the S3 putObject permission
          - s3:PutObjectAcl # it also needs ACL permission because we set it in the params (get-upload-url.js/s3.getSignedUrl('putObject', params))
        Resource: !Sub ${AssetsBucket.Arn}/* # allow the function to interact with any object in the bucket

  # (17.2.1) add the yml for the lambda function that will generate a tweet `ulid` for the 3 DDB tables,
  # write to Tweets and Timelines tables, and update Users table.
  tweet:
    handler: functions/tweet.handler
    environment: # we need to transact with 3 DDB tables
      USERS_TABLE: !Ref UsersTable
      TWEETS_TABLE: !Ref TweetsTable
      TIMELINES_TABLE: !Ref TimelinesTable
    iamRoleStatements:
      # in DDB, Put means rest POST, and Update means rest PUT
      - Effect: Allow # we need to update the tweet count at UsersTable
        Action: dynamodb:UpdateItem #
        Resource: !GetAtt UsersTable.Arn
      - Effect: Allow # we need to write to TweetsTable and TimelinesTable
        Action: dynamodb:PutItem
        Resource:
          - !GetAtt TweetsTable.Arn
          - !GetAtt TimelinesTable.Arn

  # (35.1) add a new function for retweet
  # We need to add an entry to the `TweetsTable` for the retweet, which means we need a tweetId, which is a `ulid`
  # ulid requires us to use a lambda resolver. Similar to (17.2).
  # Get from Tweets, Update Tweets and Users, write to Tweets, Timelines, Retweets,
  retweet:
    handler: functions/retweet.handler
    environment:
      USERS_TABLE: !Ref UsersTable
      TWEETS_TABLE: !Ref TweetsTable
      TIMELINES_TABLE: !Ref TimelinesTable
      RETWEETS_TABLE: !Ref RetweetsTable
    iamRoleStatements:
      - Effect: Allow
        Action: dynamodb:GetItem
        Resource: !GetAtt TweetsTable.Arn
      - Effect: Allow
        Action: dynamodb:UpdateItem
        Resource:
          - !GetAtt TweetsTable.Arn
          - !GetAtt UsersTable.Arn
      - Effect: Allow
        Action: dynamodb:PutItem
        Resource:
          - !GetAtt TweetsTable.Arn
          - !GetAtt TimelinesTable.Arn
          - !GetAtt RetweetsTable.Arn

  # (39.2) add a new function for unretweet, almost the same as retweet at (35.1)
  unretweet:
    handler: functions/unretweet.handler
    environment:
      USERS_TABLE: !Ref UsersTable
      TWEETS_TABLE: !Ref TweetsTable
      TIMELINES_TABLE: !Ref TimelinesTable
      RETWEETS_TABLE: !Ref RetweetsTable
    iamRoleStatements:
      - Effect: Allow
        Action: dynamodb:GetItem
        Resource: !GetAtt TweetsTable.Arn
      # we have to query DDB for the retweet so that we can delete it
      # we use CloudFormation's !Sub to interpolate the ARN of the table
      - Effect: Allow
        Action: dynamodb:Query
        Resource: !Sub '${TweetsTable.Arn}/index/retweetsByCreator'
      - Effect: Allow
        Action: dynamodb:UpdateItem
        Resource:
          - !GetAtt TweetsTable.Arn
          - !GetAtt UsersTable.Arn
      - Effect: Allow
        Action: dynamodb:DeleteItem
        Resource:
          - !GetAtt TweetsTable.Arn
          - !GetAtt TimelinesTable.Arn
          - !GetAtt RetweetsTable.Arn

  # (44.2) Add the lambda for reply
  # Get from Tweets, Update Tweets and Users, write to Tweets, Timelines
  # Similar to (35.1) retweets without the retweet table
  reply:
    handler: functions/reply.handler
    environment:
      USERS_TABLE: !Ref UsersTable
      TWEETS_TABLE: !Ref TweetsTable
      TIMELINES_TABLE: !Ref TimelinesTable
    iamRoleStatements:
      - Effect: Allow
        Action: dynamodb:GetItem
        Resource: !GetAtt TweetsTable.Arn
      - Effect: Allow
        Action: dynamodb:UpdateItem
        Resource:
          - !GetAtt TweetsTable.Arn
          - !GetAtt UsersTable.Arn
      - Effect: Allow
        Action: dynamodb:PutItem
        Resource:
          - !GetAtt TweetsTable.Arn
          - !GetAtt TimelinesTable.Arn

  # [51] Distribute tweets to followers
  # (51.1) add a new function for distributeTweets
  distributeTweets:
    handler: functions/distribute-tweets.handler
    environment:
      RELATIONSHIPS_TABLE: !Ref RelationshipsTable
      TIMELINES_TABLE: !Ref TimelinesTable
    events: # lambda triggered by a stream event
      - stream:
          type: dynamodb
          arn: !GetAtt TweetsTable.StreamArn
    # auto generated role name for function too long, so we use iamRoleStatementsName to override it
    iamRoleStatementsName: ${self:service}-${self:custom.stage}-distributeTweets
    iamRoleStatements:
      - Effect: Allow
        Action:
          - dynamodb:PutItem
          - dynamodb:DeleteItem
          - dynamodb:BatchWriteItem
        Resource: !GetAtt TimelinesTable.Arn
      - Effect: Allow
        Action: dynamodb:Query
        Resource: !Sub '${RelationshipsTable.Arn}/index/byOtherUser'

  # [54]Implement add tweets to timeline when following someone
  # (54.0)  add the lambda and enable streams on the table it's streaming from.
  distributeTweetsToFollower:
    handler: functions/distribute-tweets-to-follower.handler
    environment:
      TWEETS_TABLE: !Ref TweetsTable
      TIMELINES_TABLE: !Ref TimelinesTable
      MAX_TWEETS: '100'
    events: # lambda triggered by a stream event
      - stream:
          type: dynamodb
          arn: !GetAtt RelationshipsTable.StreamArn
    # (56) workaround to 64 character limit
    # auto generated role name for function too long, so we use iamRoleStatementsName to override it
    iamRoleStatementsName: ${self:service}-${self:custom.stage}-distributeTweetsToFollower
    iamRoleStatements:
      - Effect: Allow
        Action: dynamodb:Query
        Resource:
          - !Sub '${TweetsTable.Arn}/index/byCreator'
          - !Sub '${TimelinesTable.Arn}/index/byDistributedFrom'
      - Effect: Allow
        Action:
          - dynamodb:BatchWriteItem
          - dynamodb:PutItem
          - dynamodb:DeleteItem
        Resource: !GetAtt TimelinesTable.Arn

  # [64] Sync users and tweets to Algolia; we need to get all our DDB data into Algolia so that we can search them.
  # (64.1) Listen in on the stream of events from tweets & users tables, then sync the updates to Algolia. Similar to distributeTweets (51.1)
  syncUsersToAlgolia:
    handler: functions/sync-users-to-algolia.handler
    events:
      - stream:
          type: dynamodb
          arn: !GetAtt UsersTable.StreamArn
    iamRoleStatementsName: ${self:service}-${self:custom.stage}-syncUsersToAlgolia
    # [65] Securely handle secrets
    # (65.1) instead of using plain env vars, get the values from AWS SSM Parameter Store
    iamRoleStatements:
      - Effect: Allow
        Action: ssm:GetParameters
        Resource:
          - !Sub arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/${self:custom.stage}/algolia-app-id
          - !Sub arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/${self:custom.stage}/algolia-admin-key

  # (64.1)
  syncTweetsToAlgolia:
    handler: functions/sync-tweets-to-algolia.handler
    events:
      - stream:
          type: dynamodb
          arn: !GetAtt TweetsTable.StreamArn
    iamRoleStatementsName: ${self:service}-${self:custom.stage}-syncTweetsToAlgolia
    # [65] Securely handle secrets
    # (65.1) instead of using plain env vars, get the values from AWS SSM Parameter Store
    iamRoleStatements:
      - Effect: Allow
        Action: ssm:GetParameters
        Resource:
          - !Sub arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/${self:custom.stage}/algolia-app-id
          - !Sub arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/${self:custom.stage}/algolia-admin-key

  # [67] Implement search query
  # (67.0) add a lambda for search handler
  search:
    handler: functions/search.handler
    iamRoleStatementsName: ${self:service}-${self:custom.stage}-search
    iamRoleStatements:
      - Effect: Allow
        Action: ssm:GetParameters
        Resource:
          - !Sub arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/${self:custom.stage}/algolia-app-id
          - !Sub arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/${self:custom.stage}/algolia-admin-key

resources:
  Resources:
    # [4] Save user profile on PostConfirmation
    # (4.0) Create a DynamoDB table to store user profiles
    UsersTable:
      Type: AWS::DynamoDB::Table
      Properties:
        BillingMode: PAY_PER_REQUEST
        KeySchema:
          - AttributeName: id
            KeyType: HASH
        AttributeDefinitions:
          - AttributeName: id
            AttributeType: S
          # (49.1) Add `screenName` as  global secondary index to `UsersTable`
          - AttributeName: screenName
            AttributeType: S
        GlobalSecondaryIndexes:
          - IndexName: byScreenName
            KeySchema:
              - AttributeName: screenName
                KeyType: HASH
            Projection:
              ProjectionType: ALL
        StreamSpecification:
          StreamViewType: NEW_AND_OLD_IMAGES
        Tags:
          - Key: Environment
            # (4.0) Environment is dev, unless we pass in a stage override; npm run sls -- -s prod
            Value: ${self:custom.stage}
            # a helper tag to aid in monitoring the individual cost of dynamodb tables
          - Key: Name
            Value: users-table

    # [17] Implement tweet mutation
    # (17.0) Create a DynamoDB table to store tweets
    TweetsTable:
      Type: AWS::DynamoDB::Table
      Properties:
        BillingMode: PAY_PER_REQUEST
        KeySchema:
          - AttributeName: id
            KeyType: HASH
        AttributeDefinitions:
          # to fetch the tweets for a particular user, we also need the DDB partition key
          - AttributeName: creator # partition key
            AttributeType: S
          - AttributeName: id # sort key
            AttributeType: S
          - AttributeName: retweetOf # (39.2) add a new index for retweets
            AttributeType: S
          - AttributeName: inReplyToTweetId
            AttributeType: S # (43.2) add a new index for replies
        GlobalSecondaryIndexes:
          - IndexName: byCreator
            KeySchema:
              - AttributeName: creator # partition key
                KeyType: HASH
              - AttributeName: id # sort key
                KeyType: RANGE
            Projection:
              ProjectionType: ALL # so that when we get the tweets, we get all items
          - IndexName: retweetsByCreator # (39.2) add a new index for retweets
            KeySchema:
              - AttributeName: creator
                KeyType: HASH
              - AttributeName: retweetOf
                KeyType: RANGE
            Projection:
              ProjectionType: ALL
          - IndexName: repliesForTweet # (43.2) add a new index for replies
            KeySchema:
              - AttributeName: inReplyToTweetId
                KeyType: HASH
              - AttributeName: id
                KeyType: RANGE
            Projection:
              ProjectionType: ALL
        # (51.0) enable Dynamo stream specification on tweets table, used to trigger a lambda function
        StreamSpecification:
          StreamViewType: NEW_AND_OLD_IMAGES
        Tags: # so that we can track the cost in AWS billing
          - Key: Environment
            # same as (4.0) Environment is dev, unless we pass in a stage override; npm run sls -- -s prod
            Value: ${self:custom.stage}
          # a helper tag to aid in monitoring the individual cost of dynamodb tables
          - Key: Name
            Value: tweets-table

    # (17.1) Create a DynamoDB table to store tweet timelines
    TimelinesTable:
      Type: AWS::DynamoDB::Table
      Properties:
        BillingMode: PAY_PER_REQUEST
        KeySchema:
          - AttributeName: userId # partition key
            KeyType: HASH
          - AttributeName: tweetId # sort key
            KeyType: RANGE
        AttributeDefinitions:
          - AttributeName: userId
            AttributeType: S
          - AttributeName: tweetId
            AttributeType: S
          # (54.1) add a global secondary index
          # for the tweets distributed from the followed user
          - AttributeName: distributedFrom
            AttributeType: S
        GlobalSecondaryIndexes:
          - IndexName: byDistributedFrom
            KeySchema:
              - AttributeName: userId
                KeyType: HASH
              - AttributeName: distributedFrom
                KeyType: RANGE
            Projection:
              ProjectionType: ALL
        Tags:
          - Key: Environment
            Value: ${self:custom.stage}
          - Key: Name
            Value: timelines-table

    # [3] Configure Cognito User Pool
    CognitoUserPool:
      Type: AWS::Cognito::UserPool
      Properties:
        AutoVerifiedAttributes:
          - email
        Policies:
          PasswordPolicy:
            MinimumLength: 8
            RequireLowercase: false
            RequireNumbers: false
            RequireUppercase: false
            RequireSymbols: false
        # (3.0) allows login via email
        UsernameAttributes:
          - email
        # (3.0) we need to know the name and associate it with cognito user pool
        # with this setting we can configure a name attribute
        Schema:
          - AttributeDataType: String
            Name: name
            Required: false
            Mutable: true
        # (4.3) Configure Cognito to call the lambda trigger function when a new user is registered.
        # We can't use the lambda function's name, because that's something local to serverless framework
        # instead we figure out the logical id sls generates for the lambda function, by using npm run sls -- package
        # which generates cloudformation template under .serverless folder. There look for ConfirmUserSignupLambdaFunction
        LambdaConfig:
          PostConfirmation: !GetAtt ConfirmUserSignupLambdaFunction.Arn

    # (4.4) We also need to give Cognito additional permissions to call the lambda function, by default it doesn't have any
    # grants CognitoUserPool the lambda:invokeFunction permission for ConfirmUserSignupLambdaFunction
    UserPoolInvokeConfirmUserSignupLambdaPermission:
      Type: AWS::Lambda::Permission
      Properties:
        Action: lambda:invokeFunction
        FunctionName: !Ref ConfirmUserSignupLambdaFunction
        Principal: cognito-idp.amazonaws.com
        SourceArn: !GetAtt CognitoUserPool.Arn

    # (3.5) We need to be logged in with Cognito to test AppSync queries.
    # Create a cognito user by hand at *CognitoUserPool / Users and Groups*. (I used my email).
    # We also need to configure a application client at *CognitoUserPool / App clients*
    # to be able to interact with the Cognito User Pool.
    # We do this by adding a resource here
    WebUserPoolClient:
      Type: AWS::Cognito::UserPoolClient
      Properties:
        UserPoolId: !Ref CognitoUserPool
        ClientName: web
        ExplicitAuthFlows:
          - ALLOW_USER_SRP_AUTH
          - ALLOW_USER_PASSWORD_AUTH
          - ALLOW_REFRESH_TOKEN_AUTH
        # with this we get a "wrong pw" in case a user doesn't exist, makes it harder for attackers to find out if the user exists
        PreventUserExistenceErrors: ENABLED

    # (15.2) create the S3 bucket env var, to help make the s3 putObject request
    AssetsBucket:
      Type: AWS::S3::Bucket
      Properties:
        AccelerateConfiguration:
          AccelerationStatus: Enabled # because we used: const s3 = new S3({useAccelerateEndpoint: true})
        CorsConfiguration: # because the UI client needs to make a request
          CorsRules:
            - AllowedMethods:
                - GET
                - PUT
              AllowedOrigins:
                - '*'
              AllowedHeaders:
                - '*'

    # [26] like mutation
    # (26.0) create a new DDB table to track which user has liked which tweet
    LikesTable:
      Type: AWS::DynamoDB::Table
      Properties:
        BillingMode: PAY_PER_REQUEST
        KeySchema:
          - AttributeName: userId # partition key
            KeyType: HASH
          - AttributeName: tweetId # sort key
            KeyType: RANGE
        AttributeDefinitions:
          - AttributeName: userId
            AttributeType: S
          - AttributeName: tweetId
            AttributeType: S
        Tags: # so that we can track the cost in AWS billing
          - Key: Environment
            # same as (4.0) Environment is dev, unless we pass in a stage override; npm run sls -- -s prod
            Value: ${self:custom.stage}
          # a helper tag to aid in monitoring the individual cost of dynamodb tables
          - Key: Name
            Value: likes-table

    # [35] Implement retweet mutation
    # (35.0) create a new DDB table to track which user has retweeted which tweet
    RetweetsTable:
      Type: AWS::DynamoDB::Table
      Properties:
        BillingMode: PAY_PER_REQUEST
        KeySchema:
          - AttributeName: userId
            KeyType: HASH
          - AttributeName: tweetId
            KeyType: RANGE
        AttributeDefinitions:
          - AttributeName: userId
            AttributeType: S
          - AttributeName: tweetId
            AttributeType: S
        Tags:
          - Key: Environment
            Value: ${self:custom.stage}
          - Key: Name
            Value: retweets-table

    # [47] Implement follow mutation
    # (47.0) create a relationships table to track which user follows/blocks/etc. who
    # sk for sort key
    RelationshipsTable:
      Type: AWS::DynamoDB::Table
      Properties:
        BillingMode: PAY_PER_REQUEST
        KeySchema:
          - AttributeName: userId
            KeyType: HASH
          - AttributeName: sk
            KeyType: RANGE
        AttributeDefinitions:
          - AttributeName: userId
            AttributeType: S
          - AttributeName: sk
            AttributeType: S
          - AttributeName: otherUserId
            AttributeType: S
        GlobalSecondaryIndexes:
          - IndexName: byOtherUser
            KeySchema:
              - AttributeName: otherUserId
                KeyType: HASH
              - AttributeName: sk
                KeyType: RANGE
            Projection:
              ProjectionType: ALL
        # (54.0) add the lambda and enable streams on the table it's streaming from.
        StreamSpecification:
          StreamViewType: NEW_AND_OLD_IMAGES
        Tags:
          - Key: Environment
            Value: ${self:custom.stage}
          - Key: Name
            Value: relationships-table

  Outputs:
    # (3.1) We need the CognitoUserPoolId of the CognitoUserPool as a cloud formation output,
    # We get that value with !Ref
    # lets us use process.env.COGNITO_USER_POOL_ID in the e2e test
    CognitoUserPoolId:
      Value: !Ref CognitoUserPool

    # (7) In order to work with cognito in the e2e test and simulate a user signup, we need `WebUserPoolClient` id.
    # We capture that as an output in the `serverless.yml` Outputs section,
    # similar to what we did to acquire COGNITO_USER_POOL_ID (3.1)
    # lets us use process.env.WEB_COGNITO_USER_POOL_CLIENT_ID in the e2e test
    WebCognitoUserPoolClientId:
      Value: !Ref WebUserPoolClient

    # (6) add AWS_REGION as an env var
    # Use the `${self:custom.*}` trick for AWS_REGION, because we cannot use it as lambda function level since that is specific to sls.
    AwsRegion:
      Value: ${self:custom.region}
