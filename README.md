# appsyncmasterclass-backend

## 4.1 Setup backend project

Setup a GitHub repo.

`npm init -y`

`npm install --save-dev serverless/@v2.4.0` (because
`serverless-iam-roles-per-function` is broken after sls 2.4.0, remove carets).

Create a serverless project: `npm run sls -- create -t aws-nodejs`.

Install serverless-appsync-plugin: `npm i -D serverless-appsync-plugin`. It
allows to configure our AppSync api by adding a section to `serverless.yml` file
with:

```yml
custom:
  appSync:
```

Create a separate `serverless.appsync-api.yml` file for AppSync configuration.
Reference it in the main `serverless.yml` file with:

```yml
custom:
  appSync:
    - ${file(serverless.appsync-api.yml)}
```

In `serverless.yml`, exclude `package.json` files from being bundled.

```yml
package:
  exclude:
    - package-lock.json
    - package.json
```

Begin to configure the the file
[serverless.appsync-api.yml](./serverless.appsync-api.yml) (take a look).

## 4.2 Design the GraphQL schema

[4.2] Create the file [schema.api.gaphql](./schema.api.graphql). (Take a look at
the notes there).

It is very much like a TS file with types.

Identify and implement the schema; Queries, Mutations, types and interfaces that
will be used in the system.

Use interface to solidify the common properties between types (MyProfile vs
OtherProfile).

## 4.3 Configure Cognito User Pool

_(4.3.0)_ Before the GraphQL schema can be deployed, we need to create a AWS
Cognito User Pool and associate it with our AppSync API configuration. This is
done under `resources` section of [serverless.yml](./serverless.yml):

```yml
resources:
  Resources:
    CognitoUserPool:
```

(_4.3.1_) We need the CognitoUserPoolId of the CognitoUserPool as a cloud
formation output.

```yml
Outputs:
  CognitoUserPoolId:
    Value: !Ref CognitoUserPool
```

_(4.3.2)_ After configuring the Cognito User Pool, we need to configure the
AppSync API to use it ([schema.api.gaphql](./schema.api.graphql)).

```yml
name: appsyncmasterclass
schema: schema.api.graphql
# configure the AppSync API to use the cognito user pool
authenticationType: AMAZON_COGNITO_USER_POOLS
userPoolConfig:
  awsRegion: eu-west-1
  defaultAction: ALLOW
  userPoolId: !Ref CognitoUserPool
```

_(4.3.4)_ Now it is time to deploy. You need to have a AWSAccessKeyId and
AWSSecretKey to configure serverless framework to deploy.

```text
# rootkey.csv file
AWSAccessKeyId=*****
AWSSecretKey=***
```

There are a few ways to
[configure serverless with aws creds](https://www.serverless.com/framework/docs/providers/aws/guide/credentials#create-an-iam-user-and-access-key).
I used the below (mind that `--` passes args to the package.json script).

```bash
npm run sls -- config credentials \
  --provider aws \
  --key **** \
  --secret ***
```

> When testing integration or e2e, if you get a nonsense Jest timeout 5000 ms
> error, the credentials must have expired. You have to renew them to get the
> tests passing. The clue is when having to `npm run deploy` and that does not
> succeed.

Then deploy with `npm run deploy`. In _AWS console / Cognito_ we find
`appsyncmasterclass` as defined in
[serverless.appsync-api.yml](./serverless.appsync-api.yml)

_(4.3.5)_ We need to be logged in with Cognito to test AppSync queries. Create a
cognito user by hand at _CognitoUserPool / Users and Groups_. (I used my email).

We also need to configure a application client at _CognitoUserPool / App
clients_ to be able to interact with the Cognito User Pool. We do this by adding
a resource to [serverless.yml](./serverless.yml) (as opposed to doing it by hand
at AWS Console):

```yml
resources:
  Resources:
    CognitoUserPool: ##
    WebUserPoolClient:
```

_(4.3.6)_ At AWS AppSync, _Login via Cognito User Pools_ and test out some
queries.

## 4.4 Save user profile on PostConfirmation

- Capture the new user that gets created in Cognito.

- Save the user in a DynamoDB table:
  - (use a lambda trigger at _CognitoUserPool / Triggers_). After a user is
    confirmed, send a message to a lambda function, and that function can save
    the user in the DynamoDB table.
- That will allow us to use AppSync query and mutations

_(4.4.0)_ Create a DynamoDB table to store user profiles

```yml
resources:
  Resources:
    UsersTable:
    CognitoUserPool: ##
    WebUserPoolClient: ##
```

> Convention: _(4.4.0.1)_ Environment is dev, unless we pass in a stage override
> with `npm run sls -- -s prod`
>
> ```yml
> custom:
>   # Environment is dev, unless we pass in a stage override
>   stage: ${opt:stage, self:provider.stage}
>   appSync:
>     - ${file(serverless.appsync-api.yml)}
> ```

_(4.4.1)_ Add a functions block for the lambda trigger function

The function needs to know the name of the UsersTable, which is generated by
CloudFormation.

_(4.4.2)_ Install `npm i -D serverless-iam-roles-per-function` , which allows
custom permissions per function. The function needs the permission to write to
the UsersTable. We do not want a global `iamRoleStatements:` under `provider:` ,
we just want permission for this function. We use
`npm i -D serverless-iam-roles-per-function` to do this.

```yml
confirmUserSignup:
  handler: functions/confirm-user-signup.handler
  # name of the UsersTable fn is accessing
  environment:
    USERS_TABLE: !Ref UsersTable
  # custom permission for the function
  iamRoleStatements:
    - Effect: Allow
      Action:
        - dynamodb:PutItem
      Resource: !GetAtt UsersTable.Arn
```

_(4.4.3)_ Configure Cognito to call the above lambda trigger function when a new
user is registered. We can't use the lambda function's name, because that's
something local to serverless framework. Instead we figure out the logical id
sls generates for the lambda function, by using `npm run sls -- package`. Which
generates cloudformation template under .serverless folder. There look for
`ConfirmUserSignupLambdaFunction`

```yml
CognitoUserPool:
  Type: AWS::Cognito::UserPool
  Properties:
    LambdaConfig:
    PostConfirmation: !GetAtt ConfirmUserSignupLambdaFunction
```

_(4.4.4)_ We also need to give Cognito additional permissions to call the lambda
function, by default it doesn't have any. The below grants CognitoUserPool the
lambda:invokeFunction permission for ConfirmUserSignupLambdaFunction.

```yml
UserPoolInvokeConfirmUserSignupLambdaPermission:
  Type: AWS::Lambda::Permission
  Properties:
    Action: lambda:invokeFunction
    FunctionName: !Ref ConfirmUserSignupLambdaFunction
    Principal: cognito-idp.amazonaws.com
    SourceArn: !GetAtt CognitoUserPool.Arn
```

_(4.4.5)_ Now we add the lambda function
[./functions/confirm-user-signup.js](./functions/confirm-user-signup.js)

## 4.5 Testing overview

With serverless apps, unit tests do not give enough confidence for the cost.
Same cost & little value vs integration tests. Apply the test honeycomb, prefer
integration tests over unit tests, and some e2e. All because many things can go
wrong, none of which are related to our lambda code.

Unit test covers the business
logic.![unit-test](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ckgcm75wpg1ezpk5cqpr.png)

Integration is the same cost, and more value than unit. Covers the business
logic + DynamoDB
interaction.![integration-described](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/irn19obybd4dfs9bni74.png)There
are things integration tests cannot cover, but they are a good bang for the
buck.![integration](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/gtkxvl1yh7fqwahptxfa.png)

E2e can cover everything, highest confidence but also costly. We need
some.![e2e-described](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1vtufpqa62fdgprlqt6c.png)

![e2e](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qjra5fzp7yr31r06dfzd.png)

Prop-tips from Yan:

- Avoid local simulation (e.g. LocalStack), theyâ€™re more work than is worth it,
  and hides common failure modes such as misconfigured permissions and resource
  policies.
- In integration tests, only use mocks for AWS services to simulate
  hard-to-reproduce **failure cases**. If it's happy path, do not mock AWS. You
  can mock your internal services/APIs.
- Use temporary stacks for feature branches to avoid destabilizing shared
  environments, and during CI/CD pipeline to run end-to-end tests to remove the
  overhead of cleaning up test data.
  https://theburningmonk.com/2019/09/why-you-should-use-temporary-stacks-when-you-do-serverless/

## 4.6 Integration testing `confirm-user-signup`

The pattern is as follows:

- Create an event: an object which includes user info.
- Feed it to the handler (the handler causes a write to DDB, hence the
  "integration")
- Check that the result matches the expectation (by reading from DDB, hence
  "integration")

Use the `serverless-export-env` plugin to create a `.env` file with our env
vars. It picks up a few values from `serverless.yml`.

```bash
npm i -D jest @types/jest dotenv

# add it as a plugin to serverless.yml
# later version does not download COGNITO_USER_POOL_ID USERS_TABLE
npm i -D serverless-export-env@v1.4.0
npm run sls -- export-env
```

Add AWS_REGION and USER_POOL_ID to Outputs, so that they can also be acquired
via the plugin. Use the `${self:custom.*}` trick for AWS_REGION, because we
cannot use it as lambda function level since that is specific to sls.

```yml
# serverless.yml
provider:
  environment:
    STAGE: # picks up
    AWS_NODEJS_CONNECTION_REUSE_ENABLED: # picks up

custom:
  # (4.6) add AWS_REGION as an env var (use region from CLI command override, otherwise provider:region:)
  region: ${opt:region, self:provider.region}
  stage: ${opt:stage, self:provider.stage}
  appSync: ${file(serverless.appsync-api.yml)}

functions:
  confirmUserSignup:
    handler: #
    environment:
      USERS_TABLE: # picks up

  Outputs:
    CognitoUserPoolId: # picks it up as an env var too
      Value: !Ref CognitoUserPool
    # add AWS_REGION as an env var
    AwsRegion:
      Value: ${self:custom.region}
```

After the `serversless.yml` change, we have to deploy and run
`npm run sls -- export-env` again. Finally, we have an `.env` file with 5
values:

```dotenv
# .env
STAGE=dev
AWS_NODEJS_CONNECTION_REUSE_ENABLED=1
COGNITO_USER_POOL_ID=eu-west-1_***
AWS_REGION=eu-west-1
USERS_TABLE=appsyncmasterclass-backend-dev-UsersTable-***
```

Take a look at
[./**tests**/confirm-user-signup-integration.test.js](./__tests__/confirm-user-signup-integration.test.js).

## 4.7 E2e test `confirm-user-signup`

In the test there are 3 main things we do:

- We create a user from scratch using `AWS.CognitoIdentityServiceProvider`
  (cognito).
- We are not using a real email, so we use `cognito.adminConfirmSignup` to
  simulate the user sign up verification.
- As a result we should see a DynamoDB table entry, confirm it.

In order to work with cognito and simulate a user signup, we need
`WebUserPoolClient` id. We capture that as an output in the `serverless.yml `
`Outputs` section, similar to what we did to acquire _COGNITO_USER_POOL_ID
(4.3.1)_.

```yml
Outputs:
	# lets us use process.env.COGNITO_USER_POOL_ID
  CognitoUserPoolId:
    Value: !Ref CognitoUserPool
  # lets us use process.env.WEB_COGNITO_USER_POOL_CLIENT_ID
  WebUserPoolClientId:
    Value: !Ref WebUserPoolClient
```

After the `serversless.yml` change, we have to deploy `npm run deploy` and
export environment `npm run export:env`. Finally, we have an `.env` file with 6
values:

```dotenv
# .env
STAGE=dev
AWS_NODEJS_CONNECTION_REUSE_ENABLED=1
COGNITO_USER_POOL_ID=eu-west-1_***
WEB_COGNITO_USER_POOL_CLIENT_ID=******
AWS_REGION=eu-west-1
USERS_TABLE=appsyncmasterclass-backend-dev-UsersTable-***
```

Take a look at
[./**tests**/confirm-user-signup-e2e.test.js](./__tests__/confirm-user-signup-e2e.test.js).

## 4.8 Implement `getMyProfile` query (_setup an AppSync resolver and have it get an item from DDB_)

After the user is signed up and confirmed, we can get the data from DynamoDB,
similar to what we did in the integration and e2e tests.

We need to setup an AppSync resolver and have it get an item from DDB.

_(4.8.1)_ Tell the serverless AppSync plugin where the Appsync templates are
going to be, and how to map them to the graphQL query.

```yml
# serverless.appsync-api.yml
mappingTemplatesLocation: mapping-templates
mappingTemplates:
  - type: Query
    field: getMyProfile
    dataSource: usersTable # we define dataSources below for this
dataSources:
  - type: NONE
    name: none
  - type: AMAZON_DYNAMODB
    name: usersTable
    config:
      tableName: !Ref UsersTable
```

_(4.8.2)_ Per convention, add two files at the folder `./mapping-templates`,
`Query.getMyProfile.request.vtl`, `Query.getMyProfile.response.vtl` . Realize
how it matches `mappingTemplates:type&field`. Use the info in these two AWS docs
to configure the `vtl` files
[1](https://docs.aws.amazon.com/appsync/latest/devguide/resolver-mapping-template-reference-dynamodb.html),
[2](https://docs.aws.amazon.com/appsync/latest/devguide/dynamodb-helpers-in-util-dynamodb.html):

- Take the identity of the user (available in `$context.identity`), take the
  username and turn it into a DDB structure.

```vtl
// mapping-templates/Query.getMyProfile.request.vtl
{
  "version" : "2018-05-29",
  "operation" : "GetItem",
  "key" : {
    "id" : $util.dynamodb.toDynamoDBJson($context.identity.username)
  }
}
```

- For the response, turn it into json. The response is captured by AppSync into
  `$context.result`

```vtl
// mapping-templates/Query.getMyProfile.response.vtl
$util.toJson($context.result)
```

Deploy with `npm run deploy`. Verify that changes worked by looking for the
string `GraphQlResolverQuerygetMyProfile` under the templates in `.serverless`
folder

_(4.8.3)_ To test at the AWS console, we need a new Cognito user similar to the
ones created in the integration and e2e tests before. We do not have access to
those, so we use AWS CLI to create a cognito user.

`aws cognito-idp --region eu-west-1 sign-up --client-id <yourEnvVarForWebCognitoUserPoolClientId> --username <yourEmail> --password <yourPw> --user-attributes Name=name,Value=<yourName>`

Once the command goes through, we should have an unconfirmed user in the Cognito
console. Confirm the user here, it will populate in DDB - make sure you never
delete it or you have to do the steps again. Go to AppSync and sign in with the
user. Create a query for `getMyProfile` and we should see results.

![AppSyncQuery](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/7qxfzx1880j0670i33j5.png)

Try asking for the tweets field. There is no resolver associated with it, so
AppSync will return a null.

## 4.8 Unit test `getMyProfile` query

We are going to test that `Query.getMyProfile.request.vtl` executes the template
with `$context.identity.username` and turn it into a DDB json structure.

- Create an AppSync context that contains the username (for
  `$context.identity.username`).
- Get the template (file `Query.getMyProfile.request.vtl`).
- Render the template (using the utility npm packages).

`npm i -D amplify-velocity-template amplify-appsync-simulator` will help with
generating the AppSync context and rendering the `.vtl` template.

Check out `__tests__/unit/Query.getMyProfile.request.test.js`.

> Yan does not recommend to unit test the VTL template, because it
> straightforward, and in real life things do not go wrong there. In most cases
> we use AppSync to talk to DDB, and we are taking one of the examples from
> resolver mapping references
> ([1](https://docs.aws.amazon.com/appsync/latest/devguide/resolver-mapping-template-reference-dynamodb.html),
> [2](https://docs.aws.amazon.com/appsync/latest/devguide/dynamodb-helpers-in-util-dynamodb.html)).
> Therefore , instead of unit, he recommends to focus on testing e2e.

## 4.9 & 4.10 E2e test `getMyProfile` query

As a signed in user, make a graphQL request with the query `getMyProfile`.

- Sign in
- Make a graphQL request with the query
- Confirm that the returned profile is in the shape of the query.

Check out `__tests__/e2e/user-profile.test.js`.

### Getting the GraphQL API_URL

A crude way to get the GraphQLApiUrl is through the web console:
`CloudFormation/Stacks/appsyncmasterclass-backend-dev` > Outputs.

`serverless-export-env` looks at the `Outputs` property of the `serverless.yml`,
it cannot acquire `.Arn` from our AWS stack(comes as [Object object])

```yml
  Outputs:
		ConitoUserPoolArn:
		  # Getting the .Arn will not work
		  Value : !Ref CognitoUserPool.Arn

    CognitoUserPoolId:
      Value: !Ref CognitoUserPool

    WebCognitoUserPoolClientId:
      Value: !Ref WebUserPoolClient

    AwsRegion:
      Value: ${self:custom.region}
```

[4.10] To get the GraphQL API_URL from `CognitoUserPoolArn` we can use
`npm i -D serverless-manifest-plugin`. Run the command
`npm run sls -- manifest`. As opposed to looking at `serverless.yml`'s `Output`
, it looks at the CloudFormation stack that has been deployed. It outputs a
succinct json at `./.serverless/manifest.json`. We could also get the value from
there, but that's not automated.

Under `serverless.yml / custom` create a manifest section:

```yml
custom:
  ##
  manifest:
    postProcess: ./processManifest.js
    disablePostDeployGeneration: true
    disableOutput: true
    silent: true
```

Create the file `./processManifest.js`. This script is analyzes the
`manifest.json` file, looks for `outputs/OutpuKey/GraphQlApiUrl` and puts it
into the `.env` file.

```js
const _ = require('lodash')
const dotenv = require('dotenv')
const fs = require('fs')
const path = require('path')
const {promisify} = require('util')

module.exports = async function processManifest(manifestData) {
  const stageName = Object.keys(manifestData)
  const {outputs} = manifestData[stageName]

  const getOutputValue = key => {
    console.log(`loading output value for [${key}]`)
    const output = _.find(outputs, x => x.OutputKey === key)
    if (!output) {
      throw new Error(`No output found for ${key}`)
    }
    return output.OutputValue
  }

  const dotEnvFile = path.resolve('.env')
  await updateDotEnv(dotEnvFile, {
    API_URL: getOutputValue('GraphQlApiUrl'),
  })
}

/* Utils, typically this would be a package includes from NPM */
async function updateDotEnv(filePath, env) {
  // Merge with existing values
  try {
    const existing = dotenv.parse(
      await promisify(fs.readFile)(filePath, 'utf-8'),
    )
    env = Object.assign(existing, env)
  } catch (err) {
    if (err.code !== 'ENOENT') {
      throw err
    }
  }

  const contents = Object.keys(env)
    .map(key => format(key, env[key]))
    .join('\n')
  await promisify(fs.writeFile)(filePath, contents)

  return env
}

function escapeNewlines(str) {
  return str.replace(/\n/g, '\\n')
}

function format(key, value) {
  return `${key}=${escapeNewlines(value)}`
}
```

Modify the `package.json` script to also include `sls manifest`

` export:env": "sls export-env && sls manifest",`

Run the command `npm run export:env`. `API_URL=******` should get generated

```
STAGE=dev
AWS_NODEJS_CONNECTION_REUSE_ENABLED=1
COGNITO_USER_POOL_ID=eu-west-1_***
WEB_COGNITO_USER_POOL_CLIENT_ID=******
AWS_REGION=eu-west-1
USERS_TABLE=appsyncmasterclass-backend-dev-UsersTable-***
API_URL=******
```

> Make sure to clean up
> [DDB](https://eu-west-1.console.aws.amazon.com/dynamodbv2/home?region=eu-west-1#item-explorer?initialTagKey=&table=appsyncmasterclass-backend-dev-UsersTable-YMVROSIOQDW5)
> and
> [CognitoUserPool](https://eu-west-1.console.aws.amazon.com/cognito/users/?region=eu-west-1#/pool/eu-west-1_LYIK8FuXA/users?_k=zqpvnh)
> at the end of the e2e test, do not delete your `protonmail` user which is used
> in AppSync console tests.

## 4.11 Implement `editMyProfile` query (_setup an AppSync resolver and have it edit an item at DDB._)

_(4.11.0)_ Add an entry to the mapping templates

```yml
# ./serverless.appsync-api.yml
mappingTemplates:
  - type: Query
    field: getMyProfile
    dataSource: usersTable
  - type: Mutation
    field: editMyProfile
    dataSource: usersTable
```

_(4.11.1)_ We are going to write a resolver that updates the DDB usersTable. Add
the two files under `mapping-templates` folder
`Mutation.editMyProfile.request.vtl` and `Mutation.editMyProfile.response.vtl`.
Take a look at PutItem reference from AWS AppSync docs
([1](https://docs.aws.amazon.com/appsync/latest/devguide/resolver-mapping-template-reference-dynamodb.html)).
For `key:id` we use the `$util` as we did in the `getMyProfile` query. For
`attributeValues` be careful not to use
[dynamo db reserved words](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ReservedWords.html),
and if so, use an expressionNames; `name` -> `#name`, `location` -> `#location`.
Replicate the fields from `schema.api.graphql` into `expression` and
`expressionValues`, they will all be `$context.arguments.newProfile` because of
our GraphQL schema that was defined. Add a `condition`
`"expression" : "attribute_exists(id)"`, so if the user's id does not exist, the
operation fails.

```graphql
# ./schema.api.graphql
type Mutation {
  editMyProfile(newProfile: ProfileInput!): MyProfile!
  ...
}

input ProfileInput {
  name: String!
  imageUrl: AWSURL
  backgroundImageUrl: AWSURL
  bio: String
  location: String
  website: AWSURL
  birthdate: AWSDate
}
```

```
# mapping-templates/Mutation.editMyProfile.request.vtl
{
  "version" : "2018-05-29",
  "operation" : "UpdateItem",
  "key": {
    "id" : $util.dynamodb.toDynamoDBJson($context.identity.username)
  },
  "update" : {
    "expression" : "set #name = :name, imageUrl = :imageUrl, backgroundImageUrl = :backgroundImageUrl, bio = :bio, #location = :location, website = :website, birthdate = :birthdate",
    "expressionNames" : {
      "#name" : "name",
      "#location" : "location"
    },
    "expressionValues" : {
      ":name" : $util.dynamodb.toDynamoDBJson($context.arguments.newProfile.name),
      ":imageUrl" : $util.dynamodb.toDynamoDBJson($context.arguments.newProfile.imageUrl),
      ":backgroundImageUrl" : $util.dynamodb.toDynamoDBJson($context.arguments.newProfile.backgroundImageUrl),
      ":bio" : $util.dynamodb.toDynamoDBJson($context.arguments.newProfile.bio),
      ":location" : $util.dynamodb.toDynamoDBJson($context.arguments.newProfile.location),
      ":website" : $util.dynamodb.toDynamoDBJson($context.arguments.newProfile.website),
      ":birthdate" : $util.dynamodb.toDynamoDBJson($context.arguments.newProfile.birthdate)
    }
  },
  "condition" : {
    "expression" : "attribute_exists(id)"
  }
}
```

`editMyProfile.response` is the same as `getMyProfile.response`

```
# ./mapping-templates/Mutation.editMyProfile.response.vtl
$util.toJson($context.result)
```

Deploy and test at AppSync web console. If getQuery is broken, you may have
moved `chance` package to devDependencies. If Put is broken, you may have
deleted the user from DDB, and you have to re-create it as in section 4.8 using
`aws cognito-idp `.

![UpdateItem](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/mp550m5vmaatz9mk0t0h.png)

## 4.12 Unit & e2e test `editMyProfile`

### Unit

We are going to test that `Mutation.editMyProfile.request.vtl` executes the
template with `$context.identity.username` and turns it into a DDB json
structure.

- Create an AppSync context that contains the username (for
  `$context.identity.username`). KEY: when generating the context we need to
  give it an argument (compared to getMyProfile).
- Get the template (file `Mutation.editMyProfile.request.vtl`).
- Render the template (using the utility npm packages).

Check out `__tests__/unit/Mutation.editMyProfile.request.test.js`.

> Yan does not recommend to unit test the VTL template, because it
> straightforward, and in real life things do not go wrong there. In most cases
> we use AppSync to talk to DDB, and we are taking one of the examples from
> resolver mapping references
> ([1](https://docs.aws.amazon.com/appsync/latest/devguide/resolver-mapping-template-reference-dynamodb.html),
> [2](https://docs.aws.amazon.com/appsync/latest/devguide/dynamodb-helpers-in-util-dynamodb.html)).
> Therefore , instead of unit, he recommends to focus on testing e2e.

### E2e

As a signed in user, make a graphQL request with the query `editMyProfile`.

- Sign in
- Make a graphQL request with the query and variable
- Confirm that the returned profile has been edited

Check out `__tests__/e2e/user-profile.test.js`.

For the types, there are 3 key pieces of info:

- `editMyProfile` takes `newProfile` as an argument

```
# schema.api.graphql
type Mutation {
  editMyProfile(newProfile: ProfileInput!): MyProfile!
```

- At the AppSync web console we build an example

```
mutation MyMutation {
  editMyProfile(newProfile: {name: "Murat Ozcan"}) {
    id
    name
    screenName
    birthdate
    createdAt
    backgroundImageUrl
    bio
  }
}
```

- In the test, we can take an input as a parameter

```javascript
const editMyProfile = `mutation editMyProfile($input: ProfileInput!) {
	editMyProfile(newProfile: $input) {
```

And the input can be just `input: {name: newName}`.

## 4.13 Implement getImageUploadUrl query (_use a lambda to upload a file to S3_)

_(4.13.0)_ Add an entry to the mapping templates, and a dataSource. For lambda
functions, Appsync has a direct resolver integration, so we do not need a custom
request & response vtl template. Set request and response to false and
`serverless-appsync-plugin` takes care of it. When dealing with DDB, we could
leave them out because we specified the vtl files under `./mapping-templates`
and the plugin took care of it.

```yml
# ./serverless.appsync-api.yml

mappingTemplates:
	# [4.8] Implement getMyProfile query.
	# We need to setup an AppSync resolver and have it get an item from DDB.
  - type: Query
    field: getMyProfile
    dataSource: usersTable

  # [4.11] Implement editMyProfile query.
  # We need to setup an AppSync resolver and have it edit an item at DDB.
  - type: Mutation
    field: editMyProfile
    dataSource: usersTable

  # [4.13] Implement getImageUploadUrl query (use a lambda to upload a file to S3)
  - type: Query
    field: getImageUploadUrl
    dataSource: getImageUploadUrlFunction  # we define dataSources below for this
    # For lambda functions, Appsync has a direct resolver integration,
    # so we do not need a custom request & response vtl template.
    # this is how we configure it, and serverless-appsync-plugin takes care of it
    request: false
    response: false

dataSources:
  - type: NONE
    name: none
  - type: AMAZON_DYNAMODB # (4.8.1, 4.11.0)
    name: usersTable
    config:
      tableName: !Ref UsersTable
  - type: AWS_LAMBDA # (4.13.0)
    name: getImageUploadUrlFunction
    config:
      functionName: getImageUploadUrl
```

_(4.13.1)_ add the lambda function that will do the work (getImageUploadUrl)

```yml
# ./serverless.yml

functions:
  confirmUserSignup: ##

  getImageUploadUrl:
    handler: functions/get-upload-url.handler
```

Run `npm run sls --package` to test that it works so far.

### _(4.13.2)_ Implement the lambda function `functions/get-upload-url.js`.

We need to make a `putObject` request to S3. From the graphQL schema
`getImageUploadUrl(extension: String, contentType: String)` , we know that we
need an extension and contentType as args, both of which are optional. We can
get them from `event.arguments`.

For S3 `putObject` we need `key`, `contentType` and the bucket env var.

_(4.13.2.1)_ To construct the `key` for S3, we can use `event.identity.username`
(Lumigo screenshot)

![construct-s3-key](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/v9m1cqkpuwuo9gck2u55.png)

_(4.13.2.2)_ To get the `contentType` we use `event.arguments.contentType `.

_(4.13.2.3)_ create the S3 bucket env var, to help make the s3 putObject
request.

For the bucket env var, we have to add an entry to `serverless.yml` `resources`
section:

```yml
# ./serverless.yml
functions:
  confirmUserSignup: #

  getImageUploadUrl:
    handler: functions/getImageUploadUrl.handler
    environment: # (4.13.2)
      BUCKET_NAME: !Ref AssetsBucket
    iamRoleStatements:
      - Effect: Allow
        Action:
          - s3:PutObject # the lambda needs the S3 putObject permission
          # it also needs ACL permission because we set it in the params
          # get-upload-url.js/s3.getSignedUrl('putObject', params)
          - s3:PutObjectAcl
        # allow the function to interact with any object in the bucket
        Resource: !Sub ${AssetsBucket.Arn}/*

resources:
  Resources:
    UsersTable: #
    CognitoUserPool: #
    UserPoolInvokeConfirmUserSignupLambdaPermission: #
    WeUserPoolClient: #

    # (4.13.2) acquire the S3 bucket env var, to help make the s3 putObject request
    AssetsBucket:
      Type: AWS::S3::Bucket
      Properties:
        AccelerateConfiguration:
          # because we used: const s3 = new S3({useAccelerateEndpoint: true})
          AccelerationStatus: Enabled
        CorsConfiguration: # because the UI client needs to make a request
          CorsRules:
            - AllowedMethods:
                - GET
                - PUT
              AllowedOrigins:
                - '*'
              AllowedHeaders:
                - '*'
```

Other notes:

- When creating urls for the user to upload content, use S3 Transfer
  Acceleration.

- `npm i -D ulid`, and use `ulid` to create a randomized, but sorted ids.
  Problem with `chance` is the random ids are not sortable, ulid generates
  sortable keys.
- If we need to customize the file upload (ex: file size limit) we can use
  `s3.createPresignedPost` instead of `s3.getSignedUrl`. Check out Zac Charles'
  post on S3 presigned URLs vs presigned POSTs
  [here](https://medium.com/@zaccharles/s3-uploads-proxies-vs-presigned-urls-vs-presigned-posts-9661e2b37932),
  and the
  [official AWS documentation](https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-HTTPPOSTConstructPolicy.html)
  on creating POST policies (including a list of conditions you can apply).

```js
// ./functions/get-upload-url.js

// [4.13.2] Implement the lambda function. We need to make a `putObject` request to S3.
const S3 = require('aws-sdk/clients/s3')
// when creating urls for the user to upload content, use S3 Transfer Acceleration
const s3 = new S3({useAccelerateEndpoint: true})
const ulid = require('ulid')

const handler = async event => {
  // (4.13.2.1) construct the key for S3 putObject request
  // use ulid to create a randomized, but sorted id (chance is not sorted when we create multiple ids)
  const id = ulid.ulid()
  // construct a S3 key using the Construct a S3 key using the event.identity.username (got it from Lumigo)
  let key = `${event.identity.username}/${id}`
  // get the extension from graphQL schema : getImageUploadUrl(extension: String, contentType: String): AWSURL!
  const extension = event.arguments.extension
  // extension is optional, and we need to add a dot if there isn't one
  if (extension) {
    if (extension.startsWith('.')) {
      key += extension
    } else {
      key += `.${extension}`
    }
  }

  // (4.13.2.2) get the contentType from event.arguments.contentType
  // get the contentType from graphQL schema as well, it is optional so we give it a default value
  const contentType = event.arguments.contentType || 'image/jpeg'
  if (!contentType.startsWith('image/')) {
    throw new Error('contentType must start be an image')
  }

  // [4.13.2] use S3 to upload an image to S3. The operation is `putObject`
  const params = {
    Bucket: process.env.BUCKET_NAME, // (4.13.2.3) get the bucket env var (settings in serverless.yml file)
    Key: key,
    ACL: 'public-read',
    ContentType: contentType,
  }
  // note that s3.getSignedUrl is completely local, does not make a request to S3 (no need for a promise)
  return s3.getSignedUrl('putObject', params)
}

module.exports = {
  handler,
}
```

`npm run deploy` and `npm run export:env` to see the `BUCKET_NAME` populate in
the `.env` file.

## 4.14 Unit test `getImageUploadUrl`

Similar to section 4.6 `confirm-user-signup-integration.test.js`, we need to:

- Create a mock event (an object)
- Feed it to the handler
- Check that the result matches the expectation (the handler creates a certain
  S3 url)

Since there is no DDB interaction as in 4.6, or any interaction with the S3
bucket, this one is a unit test.

Check out `__tests__/unit/get-upload-url.test.js`.

## 4.15 E2e test `getImageUploadUrl`

As a signed in user, make a graphQL request with the query `getImageUploadUrl`.
Upload an image to the S3 bucket.

- Sign in.
- Make a graphQL request with the query and variables to get a signed S3 URL.
- Confirm that the upload url exists, and upload can happen.

Check out `__tests__/e2e/image-upload.test.js`.

For the types, there are 3 key pieces of info:

- `getImageUploadUrl` takes `extension` and `contentType` as an arguments:

```
# schema.api.graphql
type Query {
  getImageUploadUrl(extension: String, contentType: String): AWSURL!
```

- At the AppSync web console we build an example

```
query MyQuery {
  getImageUploadUrl(extension: ".png", contentType: "image/png")
}
```

- In the test, we can take the 2 inputs as a parameters

```javascript
const getImageUploadUrl = `query getImageUploadUrl($extension: String, $contentType: String) {
      getImageUploadUrl(extension: $extension, contentType: $contentType)
    }`
```

## 4.15 Implement tweet mutation

_(4.15.0)_ Create a DDB table to store tweets; `TweetsTable`.

```yml
# serverless.yml

resources:
  Resources:
    UsersTable: #
    TweetsTable:
      Type: AWS::DynamoDB::Table
      Properties:
        BillingMode: PAY_PER_REQUEST
        KeySchema:
          - AttributeName: id
            KeyType: HASH
        AttributeDefinitions:
          - AttributeName: id
            AttributeType: S
            # to fetch the tweets for a particular user,
            # we also need the DDB partition key
          - AttributeName: creator
            AttributeType: S
        GlobalSecondaryIndexes:
          - IndexName: byCreator
            KeySchema:
              - AttributeName: creator # hash key
                KeyType: HASH
              - AttributeName: id # range/sort key
                KeyType: RANGE
            Projection:
              # so that when we get the tweets, we get all items
              ProjectionType: ALL
        Tags: # so that we can track the cost in AWS billing
          - Key: Environment
            Value: ${self:custom.stage}
          - Key: Name
            Value: tweets-table
```

_(4.15.1)_ Create a DDB table to store tweet timelines; `TimelinesTable`

```yml
#serverless.yml

resources:
  Resources:
    UsersTable: #
    TweetsTable: #
    TimelinesTable:
      Type: AWS::DynamoDB::Table
      Properties:
        BillingMode: PAY_PER_REQUEST
        KeySchema:
          - AttributeName: userId # partition key
            KeyType: HASH
          - AttributeName: tweetId # sort key
            KeyType: RANGE
        AttributeDefinitions:
          - AttributeName: userId
            AttributeType: S
          - AttributeName: tweetId
            AttributeType: S
        Tags: # so that we can track the cost in AWS billing
          - Key: Environment
            Value: ${self:custom.stage}
          - Key: Name
            Value: timelines-table
```

When we create a new tweet, it gets written to the `TweetsTable` and
`TimelinesTable`. We also have to update `tweetsCount` for user profile page
(part of the `IProfile` from graphQL schema), which we track in `UsersTable`.
Having to transact with 3 tables, we could do these 3 operations in one DDB
transaction. However, what we cannot do in a DDB resolver is we cannot generate
the `ulid`s for the tweets, and for that we need to use a lambda resolver
instead.

_(4.15.2)_ Create a lambda resolver to generate a tweet `ulid`, write to
`TweetsTable`, `TimelinesTable` and update `UsersTable`.

_(4.15.2.0)_ Add the mapping template to `mappingTemplates`, we need resolvers
when we are transacting with DDB.

```yml
# ./serverless.appsync-api.yml

mappingTemplates:
	# [4.8] Implement getMyProfile query.
	# We need to setup an AppSync resolver and have it get an item from DDB.
  - type: Query
    field: getMyProfile
    dataSource: usersTable

  # [4.11] Implement editMyProfile query.
  # We need to setup an AppSync resolver and have it edit an item at DDB.
  - type: Mutation
    field: editMyProfile
    dataSource: usersTable

  # [4.13] Implement getImageUploadUrl query
  # (use a lambda to upload a file to S3)
  - type: Query
    field: getImageUploadUrl
    dataSource: getImageUploadUrlFunction
    request: false
    response: false

   # (4.15.2) Create a lambda resolver to generate a tweet `ulid`,
   #  write to TweetsTable, TimelinesTable and update `UsersTable`.
   # (4.15.2.0) Add the  mapping template
  - type: Mutation
    field: tweet
    dataSource: tweetFunction
    request: false
    response: false

 dataSources:
  - type: NONE
    name: none
  - type: AMAZON_DYNAMODB # (4.8.1, 4.11.0)
    name: usersTable
    config:
      tableName: !Ref UsersTable
  - type: AWS_LAMBDA # (4.13.0)
    name: getImageUploadUrlFunction
    config:
      functionName: getImageUploadUrl
  - type: AWS_LAMBDA # (4.15.2.0)
    name: tweetFunction
    config:
      functionName: tweet
```

_(4.15.2.1)_ add the yml for the lambda function that will generate a tweet
`ulid` for the 3 DDB tables, write to Tweets and Timelines tables, and update
Users table.

```yml
# serverless.yml
functions:
  confirmUserSignup: #
  getImageUploadUrl: #
  tweet:
    handler: functions/tweet.handler
    environment: # we need to transact with 3 DDB tables
      USERS_TABLE: !Ref UsersTable
      TWEETS_TABLE: !Ref TweetsTable
      TIMELINES_TABLE: !Ref TimelinesTable
    iamRoleStatements:
      # in DDB, Put means rest POST, and Update means rest PUT
      - Effect: Allow # we need to update the tweet count at UsersTable
        Action: dynamodb:UpdateItem #
        Resource: !GetAtt UsersTable.Arn
      - Effect: Allow # we need to write to TweetsTable and TimelinesTable
        Action: dynamodb:PutItem
        Resource:
          - !GetAtt TweetsTable.Arn
          - !GetAtt TimelinesTable.Arn
```

Poor man's enum in JS:

```javascript
// ./lib/constants.js
// poor man's enum in JS
const TweetTypes = {
  TWEET: 'Tweet',
  RETWEET: 'Retweet',
  REPLY: 'Reply',
}

module.exports = {
  TweetTypes,
}
```

_(4.15.2.2)_ Add the JS for the lambda function that will generate a tweet
`ulid` for the 3 DDB tables, write to Tweets and Timelines tables, and update
Users table.

```javascript
// ./functions/tweet.js

// (4.15.2.2) add the lambda function that will generate a tweet ulid for the 3 DDB tables,
// write to Tweets and Timelines tables, and update Users table
const DynamoDB = require('aws-sdk/clients/dynamodb')
const DocumentClient = new DynamoDB.DocumentClient()
const ulid = require('ulid')
const {TweetTypes} = require('../lib/constants')

const {USERS_TABLE, TIMELINES_TABLE, TWEETS_TABLE} = process.env

const handler = async event => {
  // we know from graphQL schema the argument text - tweet(text: String!): Tweet!
  // we can extract that from event.arguments
  const {text} = event.arguments
  // we can get the username from event.identity.username (Lumigo and before in (4.13.2.1) )
  const {username} = event.identity
  // generate a new ulid & timestamp for the tweet
  const id = ulid.ulid()
  const timestamp = new Date().toJSON()

  const newTweet = {
    // __typename helps us identify between the 3 types that implement ITweet (Tweet, Retweet, Reply)
    __typename: TweetTypes.TWEET,
    id,
    text,
    creator: username,
    createdAt: timestamp,
    replies: 0,
    likes: 0,
    retweets: 0,
  }

  // we need 3 operations; 2 writes to Tweets and Timelines tables, and and update to Users table
  await DocumentClient.transactWrite({
    TransactItems: [
      {
        Put: {
          TableName: TWEETS_TABLE,
          Item: newTweet,
        },
      },
      {
        Put: {
          TableName: TIMELINES_TABLE,
          Item: {
            userId: username,
            tweetId: id,
            timestamp,
          },
        },
      },
      {
        Update: {
          TableName: USERS_TABLE,
          Key: {
            id: username,
          },
          UpdateExpression: 'ADD tweetsCount :one',
          ExpressionAttributeValues: {
            ':one': 1,
          },
          // do not update if the user does not exist
          ConditionExpression: 'attribute_exists(id)',
        },
      },
    ],
  }).promise()

  return newTweet
}

module.exports = {
  handler,
}
```

`npm run deploy` and test the mutation at Appsync. Remember to
`npm run export:env` also.

![tweet-mutation](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/z0ezr67acgvjhw10a5vj.png)

Verify the 3 tables at DDB.

![3-tables](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vywbwq8uocjfrpxeecr6.png)

## 4.16 Integration test for tweet mutation

The pattern is as follows:

- Create an event: an object which includes `identity.username` and
  `arguments.text`.
- Feed it to the handler (the handler causes 2 writes and update to DDB, hence
  the "integration")
- Check that the result matches the expectation (by reading the 3 tables from
  DDB, hence "integration")

We have to have a real user for this integration test, but it is still an integration test given that we are feeding an event object to the handler. 

Check out `__tests__/integration/tweet-integration.test.js`.

## 4.17 E2e test for tweet mutation

As a signed in user, make a graphQL request with the mutation `tweet`. This will cause 3 db interactions. We do not have to repeat the same DB verifications as the integration test, but we can check that the outgoing mutation is of a certain shape when we make the graphQL request.

- Sign in
- Make a graphQL request with the tweet mutation and its text argument.
- Check the content of the response for the  mutation (no need to repeat the integration test DDB verifications, so long as we got a response, DDB transactions already happened).

For the types, there are 3 key pieces of info:

- `tweet` takes `text` as an argument:

```
# schema.api.graphql
type Mutation {
  tweet(text: String!): Tweet!
```

- At the AppSync web console we build an example

```
mutation MyMutation {
  tweet(text: "") {
    id
    createdAt
    text
    replies
    likes
    retweets
  }
}
```

- In the test, when building the query we can take the text argument.

```javascript
const tweet = `mutation tweet($text: String!) {
      tweet(text: $text) {
        id
        createdAt
        text
        replies
        likes
        retweets
      }
    }`
```

Check out `__tests__/e2e/tweet-e2e.test.js`.

## 4.18 Implement `getTweets` query

`getTweets` is a query from `schema.api.graphql`.

```
type Query{ 
	getTweets(userId: ID!, limit: Int!, nextToken: String): TweetsPage!
}
```

We are going to get the tweets from DDB, therefore we need the usual Appsync mapping-template yml and the vtl files query request and response.

(4.18.0) Add a mapping template to the yml.

```yml
# ./serverless.appsync-api.yml

mappingTemplates:
	# [4.8] Implement getMyProfile query.
	# We need to setup an AppSync resolver and have it get an item from DDB.
  - type: Query
    field: getMyProfile
    dataSource: usersTable

  # [4.11] Implement editMyProfile query.
  # We need to setup an AppSync resolver and have it edit an item at DDB.
  - type: Mutation
    field: editMyProfile
    dataSource: usersTable

  # [4.13] Implement getImageUploadUrl query
  # (use a lambda to upload a file to S3)
  - type: Query
    field: getImageUploadUrl
    dataSource: getImageUploadUrlFunction
    request: false
    response: false

   # (4.15.2) Create a lambda resolver to generate a tweet `ulid`,
   #  write to TweetsTable, TimelinesTable and update `UsersTable`.
   # (4.15.2.0) Add the  mapping template
  - type: Mutation
    field: tweet
    dataSource: tweetFunction
    request: false
    response: false
     
  # [4.18] Implement getTweets query
  # (4.18.0) Add the mapping template
  - type: Query
    field: getTweets
    dataSource: tweetsTable


 dataSources:
  - type: NONE
    name: none
    
  - type: AMAZON_DYNAMODB # (4.8.1, 4.11.0)
    name: usersTable
    config:
      tableName: !Ref UsersTable

  - type: AWS_LAMBDA # (4.13.0)
    name: getImageUploadUrlFunction
    config:
      functionName: getImageUploadUrl
      
  - type: AWS_LAMBDA # (4.15.2.0)
    name: tweetFunction
    config:
      functionName: tweet
      
  - type: AMAZON_DYNAMODB # (4.18.0) define a data source for the query
    name: tweetsTable
    config:
      tableName: !Ref TweetsTable
```

*(4.18.1)* Add the .vtl files under `./mapping-templates/` for the request and response.

In *(4.15.0)* we created a table for the tweets, and we identified a `GlobalSecondaryIndex` called `byCreator`. We will be using it now. We utilize the mapping template reference for DDB at [1](https://docs.aws.amazon.com/appsync/latest/devguide/resolver-mapping-template-reference-dynamodb.html), [2](https://docs.aws.amazon.com/appsync/latest/devguide/dynamodb-helpers-in-util-dynamodb.html). We can get userId (the first argument of the query)  by ` $util.dynamodb.toDynamoDBJson($context.arguments.userId)`. For the 2nd argument, `nextToken`, we can similarly use `$util.toJson($context.arguments.nextToken)`. `scanIndexForward` is synonymous to ascending order (latest tweet last), we want latest tweet first so this is set to `false`. We limit the number of tweets returned to be less than 25.

```
// Query.getTweets.request.vtl 

// this part did not work in the unit test (4.19),
#if ($context.arguments.limit > 25)
  $util.error("max limit is 25")
#end
// so we used this (4.19)
#set ($isValidLimit = $context.arguments.limit <= 25)
$util.validate($isValidLimit, "max limit is 25")

{
  "version" : "2018-05-29",
  "operation" : "Query",
  "query" : {
    "expression" : "creator = :userId",
    "expressionValues" : {
      ":userId" : $util.dynamodb.toDynamoDBJson($context.arguments.userId)
    }
  },
  "index" : "byCreator",
  "nextToken" : $util.toJson($context.arguments.nextToken),
  "limit" : $util.toJson($context.arguments.limit),
  "scanIndexForward" : false,
  "consistentRead" : false,
  "select" : "ALL_ATTRIBUTES"
}
```

From the schema we know that the query responds with a type `TweetsPage`. 

```
// schema.api.graphql
type Query{ 
	getTweets(userId: ID!, limit: Int!, nextToken: String): TweetsPage!
}

type TweetsPage {
  tweets: [ITweet!]
  nextToken: String
}
```

Because `tweets` will be an array, we extract that with `.items` in ` $util.toJson($context.result.items)`.  For `nextToken`, if the token is an empty string we want to turn it into null, so we use `defaultIfNullOrBlank`. `nexToken` maps to `nextToken`. 

```
// Query.getTweets.response.vtl

{
  "tweets": $util.toJson($context.result.items),
  "nextToken": $util.toJson($util.defaultIfNullOrBlank($context.result.nextToken, null))
}
```

At the moment we do not have the Profile structure in the Tweet object, if we look at DDB. Per the schema, that is something we want. What we have is `creator`, which is the id of the user that created the tweet.

```
// schema.api.graphql

type Tweet implements ITweet {
  id: ID!
  profile: IProfile!
  createdAt: AWSDateTime!
  text: String!
  replies: Int!
  likes: Int!
  retweets: Int!
  liked: Boolean!
  retweeted: Boolean!
}

interface IProfile {
  id: ID!
  name: String!
  screenName: String!
  imageUrl: AWSURL
  backgroundImageUrl: AWSURL
  bio: String
  location: String
  website: AWSURL
  birthdate: AWSDate
  createdAt: AWSDateTime!
  tweets: TweetsPage!
  followersCount: Int!
  followingCount: Int!
  tweetsCount: Int!
  likesCounts: Int!
}
```

![tweet-object](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/2q83kl4cwejudeame5xr.png)

*(4.18.2)* Take the `creator` id in the Tweet from DDB, and ask AppSync to read the user information from `UsersTable`, so that we can populate the user profile in the Tweet type of our schema. We do that by using nested resolvers. Create a nested field in mapping Templates.

```yml
# serverless.appsync-api.yml

mappingTemplates: 
  - type: Query ## 
  - type: Mutation ##
  # Yan recommends to organize the mapping templates by Query, Mutation and Nested fields.
  # We went by the order of lessons instead so it is easier to follow when reading the notes.
  
  # Nested fields
  
  - type: Tweet
    field: profile
    dataSource: usersTable
```

*(4.18.3)* Create the `.vtl` files `Tweet.profile.request.vtl`, `Tweet.profile.response.vtl` under `./mapping-templates/`

Since we have `creator` field in the `Tweet`, we can reference the nesting parent with `$context.source` .

> Nested resolvers can only be implemented for graphQL types, not interfaces.

```
// Tweet.profile.request.vtl

{
  "version" : "2018-05-29",
  "operation" : "GetItem",
  "key" : {
    "id" : $util.dynamodb.toDynamoDBJson($context.source.creator)
  }
}
```

From `schema.api.graphql` we see that `Profile` interface is implemented by both `MyProfile` and `OtherProfile`. We need to differentiate between the two.

```
// Tweet.profile.response.vtl

#if (!$util.isNull($context.result))
  #if ($context.result.id == $context.identity.username)
    #set ($context.result["__typename"] = "MyProfile")
  #else
    #set ($context.result["__typename"] = "OtherProfile")
  #end
#end

$util.toJson($context.result)
```

Deploy with `npm run deploy`. Test an AppSync query. We need a confirmed user from Cognito.

```
query MyQuery {
  getTweets(limit: 10, userId: "6b926c1a-6a54-42b3-9bf0-b623e54b1cf2") {
    nextToken
    tweets {
      id
      createdAt
      profile {
        id
        name
        screenName
        ... on MyProfile {
          id
          name
          followersCount
          followingCount
        }
        ... on OtherProfile {
          id
          name
          followedBy
        }
      }
      ... on Tweet {
        id
        likes
        replies
        retweets
        text
      }
    }
  }
}

```

![4-18-Appsync](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ysyu0a1p9b5hslog3nge.png)

## 4.19 Unit test `getTweets`

There is custom vtl code in `mapping-templates/Tweet.profile.response.vtl` and `Query.getTweets.request.vtl` worth unit testing.

Testing the `.vtl` file, the approach is to:

* Create an AppSync context
* Get the template
* Use `amplify-velocity-template` to render the template, given the context
* Check the result

Check out `__tests__/unit/Tweet.profile.response.test.js` and `__tests__/unit/Query.getTweets.request.test.js`.
